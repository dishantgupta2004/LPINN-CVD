{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906248f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Documents\\UNAI_Notes\\CVD-PINN-Project\\cvdenv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Documents\\UNAI_Notes\\CVD-PINN-Project\\cvdenv\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Documents\\UNAI_Notes\\CVD-PINN-Project\\cvdenv\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n",
      "TensorFlow version: 2.19.0\n",
      "TensorFlow Probability version: 0.25.0\n",
      "No GPUs available, using CPU\n",
      "\n",
      "Physical parameters initialized:\n",
      "Diffusion coefficients: D_SiH4 = 1e-05, D_Si = 5e-06, D_H2 = 4e-05, D_SiH2 = 1.5e-05\n",
      "Thermal parameters: k = 0.1, Cp = 700.0, œÅ = 1.0\n",
      "Reaction parameters: A1 = 1000000.0, E1 = 150000.0, A2 = 200000.0, E2 = 120000.0, A3 = 300000.0, E3 = 100000.0\n",
      "Gas constant: R = 8.314\n",
      "\n",
      "Domain bounds:\n",
      "x_min = 0.0\n",
      "x_max = 0.1\n",
      "y_min = 0.0\n",
      "y_max = 0.05\n",
      "t_min = 0.0\n",
      "t_max = 10.0\n",
      "\n",
      "Test input shape: (5, 3)\n",
      "Test output shape: (5, 5)\n",
      "\n",
      "Computed gradients:\n",
      "dy_dx shape: (5, 5, 3)\n",
      "y_x shape: (5, 5)\n",
      "y_y shape: (5, 5)\n",
      "y_t shape: (5, 5)\n",
      "y_xx shape: (5, 5)\n",
      "y_yy shape: (5, 5)\n",
      "\n",
      "Computed residuals:\n",
      "Residual 1 shape: (5, 1)\n",
      "Residual 2 shape: (5, 1)\n",
      "Residual 3 shape: (5, 1)\n",
      "Residual 4 shape: (5, 1)\n",
      "Residual 5 shape: (5, 1)\n",
      "\n",
      "Generated collocation points shape: (10, 3)\n",
      "Generated boundary points:\n",
      "inlet shape: (5, 3)\n",
      "substrate shape: (5, 3)\n",
      "left_wall shape: (5, 3)\n",
      "right_wall shape: (5, 3)\n",
      "Generated initial points shape: (10, 3)\n",
      "\n",
      "Entropy regularization test:\n",
      "Original loss: 1.100000023841858\n",
      "Modified loss: 1.1995666027069092\n",
      "Original gradient: 2.0\n",
      "Modified gradient: 1.040000081062317\n",
      "Updated alpha: 0.10000000149011612\n",
      "Updated beta: 10.0\n",
      "Updated alpha at halfway: 0.055000003427267075\n",
      "Updated beta at halfway: 55.0\n",
      "Updated alpha at end: 0.010000002570450306\n",
      "Updated beta at end: 100.0\n",
      "\n",
      "Notebook 1 completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import import_ipynb\n",
    "\n",
    "import Mathematical_Framework_Setup as nb1\n",
    "\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5e1f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "# Section 1: Traditional PINN Trainer\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "class TraditionalPINNTrainer:\n",
    "    \"\"\"\n",
    "    Trainer for traditional PINN with deterministic optimization\n",
    "    Used as a baseline for comparison with Entropy-Langevin approach\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, phys_params, domain_bounds):\n",
    "        \"\"\"\n",
    "        Initialize the traditional PINN trainer\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model : PINN\n",
    "            The PINN model to train\n",
    "        phys_params : CVDPhysicalParams\n",
    "            Object containing physical parameters\n",
    "        domain_bounds : dict\n",
    "            Dictionary with domain bounds\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.phys_params = phys_params\n",
    "        self.domain_bounds = domain_bounds\n",
    "        \n",
    "        # Create PDE residual calculator\n",
    "        self.pde_calculator = nb1.CVDPDE(phys_params)\n",
    "        \n",
    "        # Create data generator\n",
    "        self.data_generator = nb1.CVDDataGenerator(domain_bounds)\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        \n",
    "        # Initialize loss history\n",
    "        self.loss_history = {\n",
    "            'total': [],\n",
    "            'pde': [],\n",
    "            'bc': [],\n",
    "            'ic': []\n",
    "        }\n",
    "    \n",
    "    def compute_pde_loss(self, x_collocation):\n",
    "        \"\"\"\n",
    "        Compute PDE residual loss\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x_collocation : tf.Tensor\n",
    "            Collocation points for PDE residuals\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tf.Tensor\n",
    "            Mean squared PDE residual\n",
    "        \"\"\"\n",
    "        # Get model predictions\n",
    "        y_pred = self.model(x_collocation)\n",
    "        \n",
    "        # Get derivatives\n",
    "        derivatives = self.model.get_gradients(x_collocation, y_pred)\n",
    "        \n",
    "        # Compute PDE residuals\n",
    "        residuals = self.pde_calculator.compute_residuals(\n",
    "            x_collocation, y_pred, derivatives\n",
    "        )\n",
    "        \n",
    "        # Compute mean squared residual for each equation\n",
    "        mse_SiH4 = tf.reduce_mean(tf.square(residuals[0]))\n",
    "        mse_Si = tf.reduce_mean(tf.square(residuals[1]))\n",
    "        mse_H2 = tf.reduce_mean(tf.square(residuals[2]))\n",
    "        mse_SiH2 = tf.reduce_mean(tf.square(residuals[3]))\n",
    "        mse_T = tf.reduce_mean(tf.square(residuals[4]))\n",
    "        \n",
    "        # Combine all residuals\n",
    "        total_pde_loss = mse_SiH4 + mse_Si + mse_H2 + mse_SiH2 + mse_T\n",
    "        \n",
    "        return total_pde_loss\n",
    "    \n",
    "    def compute_bc_loss(self, boundary_points):\n",
    "        \"\"\"\n",
    "        Compute boundary condition loss\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        boundary_points : dict\n",
    "            Dictionary with boundary points for each boundary\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tf.Tensor\n",
    "            Boundary condition loss\n",
    "        \"\"\"\n",
    "        total_bc_loss = 0.0\n",
    "        \n",
    "        # Inlet boundary conditions (y = y_min)\n",
    "        inlet_points = boundary_points['inlet']\n",
    "        inlet_pred = self.model(inlet_points)\n",
    "        \n",
    "        # At inlet: SiH4 = 0.2, T = 350\n",
    "        inlet_SiH4_target = 0.2 * tf.ones_like(inlet_pred[:, 0:1])\n",
    "        inlet_T_target = 350.0 * tf.ones_like(inlet_pred[:, 4:5])\n",
    "        \n",
    "        inlet_loss = tf.reduce_mean(tf.square(inlet_pred[:, 0:1] - inlet_SiH4_target)) + \\\n",
    "                     tf.reduce_mean(tf.square(inlet_pred[:, 4:5] - inlet_T_target))\n",
    "        \n",
    "        # Substrate boundary conditions (y = y_max)\n",
    "        substrate_points = boundary_points['substrate']\n",
    "        substrate_pred = self.model(substrate_points)\n",
    "        \n",
    "        # At substrate: T = 700\n",
    "        substrate_T_target = 700.0 * tf.ones_like(substrate_pred[:, 4:5])\n",
    "        \n",
    "        substrate_loss = tf.reduce_mean(tf.square(substrate_pred[:, 4:5] - substrate_T_target))\n",
    "        \n",
    "        # Wall boundary conditions (no-flux)\n",
    "        # We'll simplify this by setting zero gradients at walls\n",
    "        # This could be implemented more rigorously with proper gradient calculations\n",
    "        \n",
    "        # Combine all boundary losses\n",
    "        total_bc_loss = inlet_loss + substrate_loss\n",
    "        \n",
    "        return total_bc_loss\n",
    "    \n",
    "    def compute_ic_loss(self, initial_points):\n",
    "        \"\"\"\n",
    "        Compute initial condition loss\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        initial_points : tf.Tensor\n",
    "            Initial condition points\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tf.Tensor\n",
    "            Initial condition loss\n",
    "        \"\"\"\n",
    "        # Get model predictions at initial points\n",
    "        initial_pred = self.model(initial_points)\n",
    "        \n",
    "        # Initial conditions:\n",
    "        # SiH4 = 0.1 (uniform low concentration)\n",
    "        # Si = 0.0 (no silicon initially)\n",
    "        # H2 = 0.0 (no hydrogen initially)\n",
    "        # SiH2 = 0.0 (no silylene initially)\n",
    "        # T = 300.0 (room temperature)\n",
    "        \n",
    "        initial_targets = tf.concat([\n",
    "            0.1 * tf.ones_like(initial_pred[:, 0:1]),  # SiH4\n",
    "            0.0 * tf.ones_like(initial_pred[:, 1:2]),  # Si\n",
    "            0.0 * tf.ones_like(initial_pred[:, 2:3]),  # H2\n",
    "            0.0 * tf.ones_like(initial_pred[:, 3:4]),  # SiH2\n",
    "            300.0 * tf.ones_like(initial_pred[:, 4:5])  # T\n",
    "        ], axis=1)\n",
    "        \n",
    "        # Compute mean squared error\n",
    "        initial_loss = tf.reduce_mean(tf.square(initial_pred - initial_targets))\n",
    "        \n",
    "        return initial_loss\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, x_collocation, boundary_points, initial_points):\n",
    "        \"\"\"\n",
    "        Perform one training step\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x_collocation : tf.Tensor\n",
    "            Collocation points for PDE residuals\n",
    "        boundary_points : dict\n",
    "            Dictionary with boundary points\n",
    "        initial_points : tf.Tensor\n",
    "            Initial condition points\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            (total_loss, pde_loss, bc_loss, ic_loss)\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Compute losses\n",
    "            pde_loss = self.compute_pde_loss(x_collocation)\n",
    "            bc_loss = self.compute_bc_loss(boundary_points)\n",
    "            ic_loss = self.compute_ic_loss(initial_points)\n",
    "            \n",
    "            # Weight the losses\n",
    "            # You may need to tune these weights for your specific problem\n",
    "            weighted_pde_loss = 1.0 * pde_loss\n",
    "            weighted_bc_loss = 10.0 * bc_loss\n",
    "            weighted_ic_loss = 10.0 * ic_loss\n",
    "            \n",
    "            # Compute total loss\n",
    "            total_loss = weighted_pde_loss + weighted_bc_loss + weighted_ic_loss\n",
    "        \n",
    "        # Compute gradients\n",
    "        gradients = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "        \n",
    "        # Apply gradients\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        \n",
    "        return total_loss, pde_loss, bc_loss, ic_loss\n",
    "    \n",
    "    def train(self, n_epochs, n_collocation_points=5000, batch_size=None, print_frequency=100):\n",
    "        \"\"\"\n",
    "        Train the PINN model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_epochs : int\n",
    "            Number of training epochs\n",
    "        n_collocation_points : int\n",
    "            Number of collocation points for PDE residuals\n",
    "        batch_size : int or None\n",
    "            Batch size for mini-batch training (None for full batch)\n",
    "        print_frequency : int\n",
    "            Frequency of printing and plotting results\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Loss history\n",
    "        \"\"\"\n",
    "        print(\"Starting traditional PINN training...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Generate training data once\n",
    "        x_collocation = self.data_generator.generate_collocation_points(n_collocation_points)\n",
    "        x_collocation = tf.convert_to_tensor(x_collocation, dtype=tf.float32)\n",
    "        \n",
    "        boundary_points = self.data_generator.generate_boundary_points(n_collocation_points // 10)\n",
    "        # Convert to tensors\n",
    "        for key in boundary_points:\n",
    "            boundary_points[key] = tf.convert_to_tensor(boundary_points[key], dtype=tf.float32)\n",
    "        \n",
    "        initial_points = self.data_generator.generate_initial_points(n_collocation_points // 10)\n",
    "        initial_points = tf.convert_to_tensor(initial_points, dtype=tf.float32)\n",
    "        \n",
    "        # Mini-batch training\n",
    "        if batch_size is not None:\n",
    "            # Implement mini-batch training with TensorFlow's Dataset API\n",
    "            dataset = tf.data.Dataset.from_tensor_slices((x_collocation,))\n",
    "            dataset = dataset.shuffle(buffer_size=n_collocation_points).batch(batch_size)\n",
    "            \n",
    "            # We'll need to modify the training loop for batching\n",
    "            # This is a simplified version without batching boundary and initial points\n",
    "            for epoch in range(n_epochs):\n",
    "                total_pde_loss = 0.0\n",
    "                num_batches = 0\n",
    "                \n",
    "                for batch in dataset:\n",
    "                    x_batch = batch\n",
    "                    \n",
    "                    # Perform training step\n",
    "                    total_loss, pde_loss, bc_loss, ic_loss = self.train_step(\n",
    "                        x_batch, boundary_points, initial_points\n",
    "                    )\n",
    "                    \n",
    "                    total_pde_loss += pde_loss\n",
    "                    num_batches += 1\n",
    "                \n",
    "                # Compute average losses\n",
    "                avg_pde_loss = total_pde_loss / num_batches\n",
    "                \n",
    "                # Update loss history\n",
    "                self.loss_history['total'].append(total_loss.numpy())\n",
    "                self.loss_history['pde'].append(avg_pde_loss.numpy())\n",
    "                self.loss_history['bc'].append(bc_loss.numpy())\n",
    "                self.loss_history['ic'].append(ic_loss.numpy())\n",
    "                \n",
    "                # Print progress\n",
    "                if (epoch + 1) % print_frequency == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    print(f\"Epoch {epoch+1}/{n_epochs}, \"\n",
    "                          f\"Loss: {total_loss.numpy():.6e}, \"\n",
    "                          f\"PDE: {avg_pde_loss.numpy():.6e}, \"\n",
    "                          f\"BC: {bc_loss.numpy():.6e}, \"\n",
    "                          f\"IC: {ic_loss.numpy():.6e}, \"\n",
    "                          f\"Time: {elapsed:.2f}s\")\n",
    "        else:\n",
    "            # Full batch training\n",
    "            for epoch in range(n_epochs):\n",
    "                # Perform training step\n",
    "                total_loss, pde_loss, bc_loss, ic_loss = self.train_step(\n",
    "                    x_collocation, boundary_points, initial_points\n",
    "                )\n",
    "                \n",
    "                # Update loss history\n",
    "                self.loss_history['total'].append(total_loss.numpy())\n",
    "                self.loss_history['pde'].append(pde_loss.numpy())\n",
    "                self.loss_history['bc'].append(bc_loss.numpy())\n",
    "                self.loss_history['ic'].append(ic_loss.numpy())\n",
    "                \n",
    "                # Print progress\n",
    "                if (epoch + 1) % print_frequency == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    print(f\"Epoch {epoch+1}/{n_epochs}, \"\n",
    "                          f\"Loss: {total_loss.numpy():.6e}, \"\n",
    "                          f\"PDE: {pde_loss.numpy():.6e}, \"\n",
    "                          f\"BC: {bc_loss.numpy():.6e}, \"\n",
    "                          f\"IC: {ic_loss.numpy():.6e}, \"\n",
    "                          f\"Time: {elapsed:.2f}s\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"Training completed in {total_time:.2f} seconds.\")\n",
    "        \n",
    "        return self.loss_history\n",
    "    \n",
    "    def save_model(self, filename):\n",
    "        \"\"\"\n",
    "        Save the trained model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filename : str\n",
    "            Filename to save the model\n",
    "        \"\"\"\n",
    "        self.model.save_weights(filename)\n",
    "    \n",
    "    def load_model(self, filename):\n",
    "        \"\"\"\n",
    "        Load a trained model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filename : str\n",
    "            Filename to load the model from\n",
    "        \"\"\"\n",
    "        self.model.load_weights(filename)\n",
    "    \n",
    "    def predict(self, x_test):\n",
    "        \"\"\"\n",
    "        Make predictions with the trained model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x_test : np.ndarray or tf.Tensor\n",
    "            Test points\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray\n",
    "            Model predictions\n",
    "        \"\"\"\n",
    "        # Convert to tensor if numpy array\n",
    "        if isinstance(x_test, np.ndarray):\n",
    "            x_test = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = self.model(x_test)\n",
    "        \n",
    "        return y_pred.numpy()\n",
    "    \n",
    "    def plot_loss_history(self):\n",
    "        \"\"\"Plot the loss history during training\"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        epochs = range(1, len(self.loss_history['total']) + 1)\n",
    "        \n",
    "        plt.semilogy(epochs, self.loss_history['total'], 'k-', linewidth=2, label='Total Loss')\n",
    "        plt.semilogy(epochs, self.loss_history['pde'], 'r-', linewidth=1.5, label='PDE Loss')\n",
    "        plt.semilogy(epochs, self.loss_history['bc'], 'b-', linewidth=1.5, label='BC Loss')\n",
    "        plt.semilogy(epochs, self.loss_history['ic'], 'g-', linewidth=1.5, label='IC Loss')\n",
    "        \n",
    "        plt.xlabel('Epochs', fontsize=14)\n",
    "        plt.ylabel('Loss', fontsize=14)\n",
    "        plt.title('Loss History (Traditional PINN)', fontsize=16)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.grid(True, which='both', linestyle='--', alpha=0.6)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(r'D:\\Documents\\UNAI_Notes\\CVD-PINN-Project\\notebooks\\figures\\traditional_pinn_loss.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0640ad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "# Section 2: Entropy-Langevin PINN Trainer\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "class EntropyLangevinPINNTrainer:\n",
    "    \"\"\"\n",
    "    Trainer implementing the Entropy-Langevin algorithm for ensemble of PINNs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, models, phys_params, domain_bounds, alpha=0.1, beta=10.0, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        Initialize the Entropy-Langevin PINN trainer\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        models : list\n",
    "            List of PINN models in the ensemble\n",
    "        phys_params : CVDPhysicalParams\n",
    "            Object containing physical parameters\n",
    "        domain_bounds : dict\n",
    "            Dictionary with domain bounds\n",
    "        alpha : float\n",
    "            Initial entropy weight parameter\n",
    "        beta : float\n",
    "            Initial inverse temperature parameter\n",
    "        learning_rate : float\n",
    "            Learning rate for optimizer\n",
    "        \"\"\"\n",
    "        self.models = models\n",
    "        self.num_models = len(models)\n",
    "        self.phys_params = phys_params\n",
    "        self.domain_bounds = domain_bounds\n",
    "        \n",
    "        # Create PDE residual calculator\n",
    "        self.pde_calculator = nb1.CVDPDE(phys_params)\n",
    "        \n",
    "        # Create data generator\n",
    "        self.data_generator = nb1.CVDDataGenerator(domain_bounds)\n",
    "        \n",
    "        # Initialize entropy regularization\n",
    "        self.entropy_reg = nb1.EntropyRegularizedLoss(alpha, beta)\n",
    "        \n",
    "        # Initialize optimizers (one for each model)\n",
    "        self.optimizers = [tf.keras.optimizers.Adam(learning_rate=learning_rate) \n",
    "                          for _ in range(self.num_models)]\n",
    "        \n",
    "        # Initialize loss history\n",
    "        self.loss_history = {\n",
    "            'total': [],\n",
    "            'pde': [[] for _ in range(self.num_models)],\n",
    "            'bc': [[] for _ in range(self.num_models)],\n",
    "            'ic': [[] for _ in range(self.num_models)],\n",
    "            'entropy': []\n",
    "        }\n",
    "    \n",
    "    def compute_pde_loss(self, model_idx, x_collocation):\n",
    "        \"\"\"\n",
    "        Compute PDE residual loss for a specific model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model_idx : int\n",
    "            Index of the model in the ensemble\n",
    "        x_collocation : tf.Tensor\n",
    "            Collocation points for PDE residuals\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tf.Tensor\n",
    "            Mean squared PDE residual\n",
    "        \"\"\"\n",
    "        # Get model\n",
    "        model = self.models[model_idx]\n",
    "        \n",
    "        # Get model predictions\n",
    "        y_pred = model(x_collocation)\n",
    "        \n",
    "        # Get derivatives\n",
    "        derivatives = model.get_gradients(x_collocation, y_pred)\n",
    "        \n",
    "        # Compute PDE residuals\n",
    "        residuals = self.pde_calculator.compute_residuals(\n",
    "            x_collocation, y_pred, derivatives\n",
    "        )\n",
    "        \n",
    "        # Compute mean squared residual for each equation\n",
    "        mse_SiH4 = tf.reduce_mean(tf.square(residuals[0]))\n",
    "        mse_Si = tf.reduce_mean(tf.square(residuals[1]))\n",
    "        mse_H2 = tf.reduce_mean(tf.square(residuals[2]))\n",
    "        mse_SiH2 = tf.reduce_mean(tf.square(residuals[3]))\n",
    "        mse_T = tf.reduce_mean(tf.square(residuals[4]))\n",
    "        \n",
    "        # Combine all residuals\n",
    "        total_pde_loss = mse_SiH4 + mse_Si + mse_H2 + mse_SiH2 + mse_T\n",
    "        \n",
    "        return total_pde_loss\n",
    "    \n",
    "    def compute_bc_loss(self, model_idx, boundary_points):\n",
    "        \"\"\"\n",
    "        Compute boundary condition loss for a specific model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model_idx : int\n",
    "            Index of the model in the ensemble\n",
    "        boundary_points : dict\n",
    "            Dictionary with boundary points for each boundary\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tf.Tensor\n",
    "            Boundary condition loss\n",
    "        \"\"\"\n",
    "        # Get model\n",
    "        model = self.models[model_idx]\n",
    "        \n",
    "        total_bc_loss = 0.0\n",
    "        \n",
    "        # Inlet boundary conditions (y = y_min)\n",
    "        inlet_points = boundary_points['inlet']\n",
    "        inlet_pred = model(inlet_points)\n",
    "        \n",
    "        # At inlet: SiH4 = 0.2, T = 350\n",
    "        inlet_SiH4_target = 0.2 * tf.ones_like(inlet_pred[:, 0:1])\n",
    "        inlet_T_target = 350.0 * tf.ones_like(inlet_pred[:, 4:5])\n",
    "        \n",
    "        inlet_loss = tf.reduce_mean(tf.square(inlet_pred[:, 0:1] - inlet_SiH4_target)) + \\\n",
    "                     tf.reduce_mean(tf.square(inlet_pred[:, 4:5] - inlet_T_target))\n",
    "        \n",
    "        # Substrate boundary conditions (y = y_max)\n",
    "        substrate_points = boundary_points['substrate']\n",
    "        substrate_pred = model(substrate_points)\n",
    "        \n",
    "        # At substrate: T = 700\n",
    "        substrate_T_target = 700.0 * tf.ones_like(substrate_pred[:, 4:5])\n",
    "        \n",
    "        substrate_loss = tf.reduce_mean(tf.square(substrate_pred[:, 4:5] - substrate_T_target))\n",
    "        \n",
    "        # Combine all boundary losses\n",
    "        total_bc_loss = inlet_loss + substrate_loss\n",
    "        \n",
    "        return total_bc_loss\n",
    "    \n",
    "    def compute_ic_loss(self, model_idx, initial_points):\n",
    "        \"\"\"\n",
    "        Compute initial condition loss for a specific model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model_idx : int\n",
    "            Index of the model in the ensemble\n",
    "        initial_points : tf.Tensor\n",
    "            Initial condition points\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tf.Tensor\n",
    "            Initial condition loss\n",
    "        \"\"\"\n",
    "        # Get model\n",
    "        model = self.models[model_idx]\n",
    "        \n",
    "        # Get model predictions at initial points\n",
    "        initial_pred = model(initial_points)\n",
    "        \n",
    "        initial_targets = tf.concat([\n",
    "            0.1 * tf.ones_like(initial_pred[:, 0:1]),  # SiH4\n",
    "            0.0 * tf.ones_like(initial_pred[:, 1:2]),  # Si\n",
    "            0.0 * tf.ones_like(initial_pred[:, 2:3]),  # H2\n",
    "            0.0 * tf.ones_like(initial_pred[:, 3:4]),  # SiH2\n",
    "            300.0 * tf.ones_like(initial_pred[:, 4:5])  # T\n",
    "        ], axis=1)\n",
    "        \n",
    "        initial_loss = tf.reduce_mean(tf.square(initial_pred - initial_targets))\n",
    "        \n",
    "        return initial_loss\n",
    "    \n",
    "    def train_step(self, epoch, x_collocation, boundary_points, initial_points):\n",
    "        \"\"\"\n",
    "        Perform one training step for the ensemble\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        epoch : int\n",
    "            Current epoch number\n",
    "        x_collocation : tf.Tensor\n",
    "            Collocation points for PDE residuals\n",
    "        boundary_points : dict\n",
    "            Dictionary with boundary points\n",
    "        initial_points : tf.Tensor\n",
    "            Initial condition points\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            (total_losses, pde_losses, bc_losses, ic_losses)\n",
    "        \"\"\"\n",
    "        # Update entropy-Langevin parameters\n",
    "        self.entropy_reg.update_parameters(epoch, self.n_epochs)\n",
    "        \n",
    "        # Initialize lists to store losses and gradients\n",
    "        total_losses = []\n",
    "        pde_losses = []\n",
    "        bc_losses = []\n",
    "        ic_losses = []\n",
    "        all_gradients = []\n",
    "        \n",
    "        # Step 1: Compute losses and gradients for all models\n",
    "        for i in range(self.num_models):\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Compute losses\n",
    "                pde_loss = self.compute_pde_loss(i, x_collocation)\n",
    "                bc_loss = self.compute_bc_loss(i, boundary_points)\n",
    "                ic_loss = self.compute_ic_loss(i, initial_points)\n",
    "                \n",
    "                # Weight the losses\n",
    "                weighted_pde_loss = 1.0 * pde_loss\n",
    "                weighted_bc_loss = 10.0 * bc_loss\n",
    "                weighted_ic_loss = 10.0 * ic_loss\n",
    "                \n",
    "                # Compute total loss (without entropy regularization for now)\n",
    "                total_loss = weighted_pde_loss + weighted_bc_loss + weighted_ic_loss\n",
    "            \n",
    "            # Store losses\n",
    "            total_losses.append(total_loss)\n",
    "            pde_losses.append(pde_loss)\n",
    "            bc_losses.append(bc_loss)\n",
    "            ic_losses.append(ic_loss)\n",
    "            \n",
    "            # Compute gradients\n",
    "            gradients = tape.gradient(total_loss, self.models[i].trainable_variables)\n",
    "            all_gradients.append(gradients)\n",
    "        \n",
    "        # Step 2: Compute ensemble average gradient\n",
    "        # This is a simplified approach; in practice, we need to handle variable shapes properly\n",
    "        avg_gradients = []\n",
    "        for var_idx in range(len(all_gradients[0])):\n",
    "            avg_grad = tf.zeros_like(all_gradients[0][var_idx])\n",
    "            for model_idx in range(self.num_models):\n",
    "                avg_grad += all_gradients[model_idx][var_idx]\n",
    "            avg_grad /= self.num_models\n",
    "            avg_gradients.append(avg_grad)\n",
    "        \n",
    "        # Step 3: Apply modified gradients with Langevin dynamics\n",
    "        for i in range(self.num_models):\n",
    "            # Apply entropy regularization to gradients\n",
    "            modified_gradients = []\n",
    "            for var_idx, grad in enumerate(all_gradients[i]):\n",
    "                # Entropy feedback term: -Œ±*Œ≤*(‚àáL - E[‚àáL])\n",
    "                entropy_feedback = -self.entropy_reg.alpha * self.entropy_reg.beta * (grad - avg_gradients[var_idx])\n",
    "                \n",
    "                # Modified gradient: ‚àáL + entropy_feedback\n",
    "                modified_grad = grad + entropy_feedback\n",
    "                \n",
    "                # Add Langevin noise: ‚àö(2Œ∑/Œ≤) * N(0,1)\n",
    "                noise_scale = tf.sqrt(2 * self.optimizers[i].learning_rate / self.entropy_reg.beta)\n",
    "                noise = noise_scale * tf.random.normal(shape=grad.shape)\n",
    "                \n",
    "                # Final modified gradient with Langevin noise\n",
    "                final_modified_grad = modified_grad + noise\n",
    "                \n",
    "                modified_gradients.append(final_modified_grad)\n",
    "            \n",
    "            # Apply gradients\n",
    "            self.optimizers[i].apply_gradients(zip(modified_gradients, self.models[i].trainable_variables))\n",
    "        \n",
    "        # Convert lists to tensors for return\n",
    "        total_losses = tf.stack(total_losses)\n",
    "        pde_losses = tf.stack(pde_losses)\n",
    "        bc_losses = tf.stack(bc_losses)\n",
    "        ic_losses = tf.stack(ic_losses)\n",
    "        \n",
    "        return total_losses, pde_losses, bc_losses, ic_losses\n",
    "    \n",
    "    def train(self, n_epochs, n_collocation_points=5000, print_frequency=100):\n",
    "        \"\"\"\n",
    "        Train the ensemble of PINN models\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_epochs : int\n",
    "            Number of training epochs\n",
    "        n_collocation_points : int\n",
    "            Number of collocation points for PDE residuals\n",
    "        print_frequency : int\n",
    "            Frequency of printing results\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Loss history\n",
    "        \"\"\"\n",
    "        print(\"Starting Entropy-Langevin PINN training...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.n_epochs = n_epochs  # Store for use in parameter scheduling\n",
    "        \n",
    "        # Generate training data once\n",
    "        x_collocation = self.data_generator.generate_collocation_points(n_collocation_points)\n",
    "        x_collocation = tf.convert_to_tensor(x_collocation, dtype=tf.float32)\n",
    "        \n",
    "        boundary_points = self.data_generator.generate_boundary_points(n_collocation_points // 10)\n",
    "        # Convert to tensors\n",
    "        for key in boundary_points:\n",
    "            boundary_points[key] = tf.convert_to_tensor(boundary_points[key], dtype=tf.float32)\n",
    "        \n",
    "        initial_points = self.data_generator.generate_initial_points(n_collocation_points // 10)\n",
    "        initial_points = tf.convert_to_tensor(initial_points, dtype=tf.float32)\n",
    "        \n",
    "        # Training loop\n",
    "        avg_total_loss = 0.0\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            # Perform one training step for all models\n",
    "            total_losses, pde_losses, bc_losses, ic_losses = self.train_step(\n",
    "                epoch, x_collocation, boundary_points, initial_points\n",
    "            )\n",
    "            \n",
    "            # Compute average losses across ensemble\n",
    "            avg_total_loss = tf.reduce_mean(total_losses)\n",
    "            avg_pde_loss = tf.reduce_mean(pde_losses)\n",
    "            avg_bc_loss = tf.reduce_mean(bc_losses)\n",
    "            avg_ic_loss = tf.reduce_mean(ic_losses)\n",
    "            \n",
    "            # Update loss history\n",
    "            self.loss_history['total'].append(avg_total_loss.numpy())\n",
    "            for i in range(self.num_models):\n",
    "                self.loss_history['pde'][i].append(pde_losses[i].numpy())\n",
    "                self.loss_history['bc'][i].append(bc_losses[i].numpy())\n",
    "                self.loss_history['ic'][i].append(ic_losses[i].numpy())\n",
    "            \n",
    "            # Print progress\n",
    "            if (epoch + 1) % print_frequency == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"Epoch {epoch+1}/{n_epochs}, \"\n",
    "                      f\"Avg Loss: {avg_total_loss.numpy():.6e}, \"\n",
    "                      f\"Avg PDE: {avg_pde_loss.numpy():.6e}, \"\n",
    "                      f\"Avg BC: {avg_bc_loss.numpy():.6e}, \"\n",
    "                      f\"Avg IC: {avg_ic_loss.numpy():.6e}, \"\n",
    "                      f\"Alpha: {self.entropy_reg.alpha.numpy():.4f}, \"\n",
    "                      f\"Beta: {self.entropy_reg.beta.numpy():.2f}, \"\n",
    "                      f\"Time: {elapsed:.2f}s\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"Training completed in {total_time:.2f} seconds.\")\n",
    "        \n",
    "        return self.loss_history\n",
    "    \n",
    "    def save_models(self, base_filename):\n",
    "        \"\"\"\n",
    "        Save all models in the ensemble\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        base_filename : str\n",
    "            Base filename to save the models\n",
    "        \"\"\"\n",
    "        for i, model in enumerate(self.models):\n",
    "            filename = f\"{base_filename}_model_{i}.h5\"\n",
    "            model.save_weights(filename)\n",
    "        \n",
    "        # Save loss history\n",
    "        with open(f\"{base_filename}_loss_history.pkl\", 'wb') as f:\n",
    "            pickle.dump(self.loss_history, f)\n",
    "    \n",
    "    def load_models(self, base_filename):\n",
    "        \"\"\"\n",
    "        Load all models in the ensemble\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        base_filename : str\n",
    "            Base filename to load the models from\n",
    "        \"\"\"\n",
    "        for i, model in enumerate(self.models):\n",
    "            filename = f\"{base_filename}_model_{i}.h5\"\n",
    "            model.load_weights(filename)\n",
    "        \n",
    "        # Load loss history if available\n",
    "        try:\n",
    "            with open(f\"{base_filename}_loss_history.pkl\", 'rb') as f:\n",
    "                self.loss_history = pickle.load(f)\n",
    "        except:\n",
    "            print(\"No loss history found.\")\n",
    "    \n",
    "    def predict_ensemble(self, x_test):\n",
    "        \"\"\"\n",
    "        Make predictions with all models in the ensemble\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x_test : np.ndarray or tf.Tensor\n",
    "            Test points\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            (mean_prediction, std_prediction)\n",
    "        \"\"\"\n",
    "        # Convert to tensor if numpy array\n",
    "        if isinstance(x_test, np.ndarray):\n",
    "            x_test = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
    "        \n",
    "        # Make predictions with all models\n",
    "        predictions = []\n",
    "        for model in self.models:\n",
    "            y_pred = model(x_test)\n",
    "            predictions.append(y_pred.numpy())\n",
    "        \n",
    "        # Stack predictions\n",
    "        predictions = np.stack(predictions, axis=0)\n",
    "        \n",
    "        # Compute mean and standard deviation\n",
    "        mean_prediction = np.mean(predictions, axis=0)\n",
    "        std_prediction = np.std(predictions, axis=0)\n",
    "        \n",
    "        return mean_prediction, std_prediction\n",
    "    \n",
    "    def plot_loss_history(self):\n",
    "        \"\"\"Plot the loss history during training\"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        epochs = range(1, len(self.loss_history['total']) + 1)\n",
    "        \n",
    "        # Plot total loss\n",
    "        plt.semilogy(epochs, self.loss_history['total'], 'k-', linewidth=2, label='Avg Total Loss')\n",
    "        \n",
    "        # Plot individual model PDE losses with transparency\n",
    "        for i in range(self.num_models):\n",
    "            plt.semilogy(epochs, self.loss_history['pde'][i], 'r-', linewidth=0.5, alpha=0.3)\n",
    "        \n",
    "        # Plot average PDE loss\n",
    "        avg_pde_loss = np.mean([self.loss_history['pde'][i] for i in range(self.num_models)], axis=0)\n",
    "        plt.semilogy(epochs, avg_pde_loss, 'r-', linewidth=1.5, label='Avg PDE Loss')\n",
    "        \n",
    "        # Plot average BC loss\n",
    "        avg_bc_loss = np.mean([self.loss_history['bc'][i] for i in range(self.num_models)], axis=0)\n",
    "        plt.semilogy(epochs, avg_bc_loss, 'b-', linewidth=1.5, label='Avg BC Loss')\n",
    "        \n",
    "        # Plot average IC loss\n",
    "        avg_ic_loss = np.mean([self.loss_history['ic'][i] for i in range(self.num_models)], axis=0)\n",
    "        plt.semilogy(epochs, avg_ic_loss, 'g-', linewidth=1.5, label='Avg IC Loss')\n",
    "        \n",
    "        plt.xlabel('Epochs', fontsize=14)\n",
    "        plt.ylabel('Loss', fontsize=14)\n",
    "        plt.title('Loss History (Entropy-Langevin PINN)', fontsize=16)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.grid(True, which='both', linestyle='--', alpha=0.6)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('figures/entropy_langevin_pinn_loss.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def visualize_predictions(self, t_idx=5, output_idx=0, nx=50, ny=50, nt=10):\n",
    "        \"\"\"\n",
    "        Visualize predictions with uncertainty quantification\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        t_idx : int\n",
    "            Time index to visualize\n",
    "        output_idx : int\n",
    "            Output index to visualize (0: SiH4, 1: Si, 2: H2, 3: SiH2, 4: T)\n",
    "        nx, ny, nt : int\n",
    "            Number of points in each dimension for visualization grid\n",
    "        \"\"\"\n",
    "        # Species names and titles\n",
    "        species_names = [\"SiH4\", \"Si\", \"H2\", \"SiH2\", \"Temperature\"]\n",
    "        \n",
    "        # Generate uniform grid for visualization\n",
    "        grid_points, grid_shape = self.data_generator.generate_uniform_grid(nx, ny, nt)\n",
    "        \n",
    "        # Make predictions\n",
    "        mean_pred, std_pred = self.predict_ensemble(grid_points)\n",
    "        \n",
    "        # Reshape predictions\n",
    "        mean_pred = mean_pred.reshape(grid_shape[0], grid_shape[1], grid_shape[2], 5)\n",
    "        std_pred = std_pred.reshape(grid_shape[0], grid_shape[1], grid_shape[2], 5)\n",
    "        \n",
    "        # Extract data for the specified time step and output\n",
    "        x = np.linspace(self.domain_bounds['x_min'], self.domain_bounds['x_max'], nx)\n",
    "        y = np.linspace(self.domain_bounds['y_min'], self.domain_bounds['y_max'], ny)\n",
    "        t = np.linspace(self.domain_bounds['t_min'], self.domain_bounds['t_max'], nt)\n",
    "        \n",
    "        # Get actual time value\n",
    "        time_val = t[t_idx]\n",
    "        \n",
    "        # Extract mean and std predictions for the specified time and output\n",
    "        mean_slice = mean_pred[:, :, t_idx, output_idx]\n",
    "        std_slice = std_pred[:, :, t_idx, output_idx]\n",
    "        \n",
    "        # Create meshgrid for plotting\n",
    "        X, Y = np.meshgrid(x, y, indexing='ij')\n",
    "        \n",
    "        # Create a 3x1 grid of plots\n",
    "        fig = plt.figure(figsize=(18, 6))\n",
    "        gs = GridSpec(1, 3, figure=fig)\n",
    "        \n",
    "        # Plot mean prediction\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        cf1 = ax1.contourf(X, Y, mean_slice, 50, cmap='viridis')\n",
    "        plt.colorbar(cf1, ax=ax1, label=f\"{species_names[output_idx]}\")\n",
    "        ax1.set_xlabel('x (m)')\n",
    "        ax1.set_ylabel('y (m)')\n",
    "        ax1.set_title(f\"Mean {species_names[output_idx]} at t = {time_val:.2f}s\")\n",
    "        \n",
    "        # Plot standard deviation\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        cf2 = ax2.contourf(X, Y, std_slice, 50, cmap='plasma')\n",
    "        plt.colorbar(cf2, ax=ax2, label=f\"Std Dev of {species_names[output_idx]}\")\n",
    "        ax2.set_xlabel('x (m)')\n",
    "        ax2.set_ylabel('y (m)')\n",
    "        ax2.set_title(f\"Uncertainty in {species_names[output_idx]} at t = {time_val:.2f}s\")\n",
    "        \n",
    "        # Plot coefficient of variation (std/mean)\n",
    "        ax3 = fig.add_subplot(gs[0, 2])\n",
    "        # Add a small epsilon to avoid division by zero\n",
    "        epsilon = 1e-10\n",
    "        cv = std_slice / (np.abs(mean_slice) + epsilon)\n",
    "        # Clip extremely high values for better visualization\n",
    "        cv = np.clip(cv, 0, 0.5)\n",
    "        cf3 = ax3.contourf(X, Y, cv, 50, cmap='hot')\n",
    "        plt.colorbar(cf3, ax=ax3, label='Coefficient of Variation')\n",
    "        ax3.set_xlabel('x (m)')\n",
    "        ax3.set_ylabel('y (m)')\n",
    "        ax3.set_title(f\"Relative Uncertainty at t = {time_val:.2f}s\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'notebooks/figures/prediction_{species_names[output_idx]}_t{t_idx}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2064616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== Training Traditional PINN ======\n",
      "Starting traditional PINN training...\n",
      "Epoch 100/1000, Loss: nan, PDE: nan, BC: nan, IC: nan, Time: 296.70s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 103\u001b[39m\n\u001b[32m     98\u001b[39m     el_trainer.visualize_predictions(t_idx=\u001b[32m5\u001b[39m, output_idx=\u001b[32m4\u001b[39m)\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m traditional_trainer \u001b[38;5;28;01mif\u001b[39;00m compare_with_traditional \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, el_trainer\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[43mdemo_entropy_langevin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_models\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mdemo_entropy_langevin\u001b[39m\u001b[34m(train_models, compare_with_traditional, n_epochs)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m train_models:\n\u001b[32m     53\u001b[39m     \u001b[38;5;66;03m# Train traditional PINN\u001b[39;00m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m====== Training Traditional PINN ======\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     traditional_loss_history = \u001b[43mtraditional_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# Save traditional model\u001b[39;00m\n\u001b[32m     58\u001b[39m     traditional_trainer.save_model(\u001b[33m\"\u001b[39m\u001b[33mnotebooks/models/traditional_pinn.h5\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 286\u001b[39m, in \u001b[36mTraditionalPINNTrainer.train\u001b[39m\u001b[34m(self, n_epochs, n_collocation_points, batch_size, print_frequency)\u001b[39m\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    283\u001b[39m     \u001b[38;5;66;03m# Full batch training\u001b[39;00m\n\u001b[32m    284\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[32m    285\u001b[39m         \u001b[38;5;66;03m# Perform training step\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m         total_loss, pde_loss, bc_loss, ic_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m            \u001b[49m\u001b[43mx_collocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboundary_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_points\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m         \u001b[38;5;66;03m# Update loss history\u001b[39;00m\n\u001b[32m    291\u001b[39m         \u001b[38;5;28mself\u001b[39m.loss_history[\u001b[33m'\u001b[39m\u001b[33mtotal\u001b[39m\u001b[33m'\u001b[39m].append(total_loss.numpy())\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\UNAI_Notes\\CVD-PINN-Project\\cvdenv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\UNAI_Notes\\CVD-PINN-Project\\cvdenv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\UNAI_Notes\\CVD-PINN-Project\\cvdenv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:869\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    866\u001b[39m   \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    867\u001b[39m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[32m    868\u001b[39m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    873\u001b[39m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[32m    874\u001b[39m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[32m    875\u001b[39m   \u001b[38;5;28mself\u001b[39m._lock.release()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\UNAI_Notes\\CVD-PINN-Project\\cvdenv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\UNAI_Notes\\CVD-PINN-Project\\cvdenv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\UNAI_Notes\\CVD-PINN-Project\\cvdenv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\UNAI_Notes\\CVD-PINN-Project\\cvdenv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\UNAI_Notes\\CVD-PINN-Project\\cvdenv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\UNAI_Notes\\CVD-PINN-Project\\cvdenv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "# Section 3: Demo and Testing\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "def create_pinn_model(hidden_layers=[64, 64, 64, 64, 64, 64], activation='tanh'):\n",
    "    \"\"\"Create a PINN model with specified architecture\"\"\"\n",
    "    model = nb1.PINN(hidden_layers, activation)\n",
    "    # Compile with dummy data to build\n",
    "    dummy_input = tf.zeros((1, 3))\n",
    "    _ = model(dummy_input)\n",
    "    return model\n",
    "\n",
    "def create_model_ensemble(num_models=10, hidden_layers=[64, 64, 64, 64, 64, 64], activation='tanh'):\n",
    "    \"\"\"Create an ensemble of PINN models\"\"\"\n",
    "    models = []\n",
    "    for i in range(num_models):\n",
    "        models.append(create_pinn_model(hidden_layers, activation))\n",
    "    return models\n",
    "\n",
    "# Demo of Entropy-Langevin training\n",
    "def demo_entropy_langevin(train_models=True, compare_with_traditional=True, n_epochs=1000):\n",
    "    \"\"\"\n",
    "    Demonstrate Entropy-Langevin PINN training compared to traditional training\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_models : bool\n",
    "        Whether to train new models or load existing ones\n",
    "    compare_with_traditional : bool\n",
    "        Whether to compare with traditional PINN training\n",
    "    n_epochs : int\n",
    "        Number of training epochs\n",
    "    \"\"\"\n",
    "    # Define domain bounds\n",
    "    domain_bounds = {\n",
    "        'x_min': 0.0,\n",
    "        'x_max': 0.1,\n",
    "        'y_min': 0.0,\n",
    "        'y_max': 0.05,\n",
    "        't_min': 0.0,\n",
    "        't_max': 10.0\n",
    "    }\n",
    "    \n",
    "    # Create physical parameters\n",
    "    phys_params = nb1.CVDPhysicalParams()\n",
    "    \n",
    "    if compare_with_traditional:\n",
    "        # Create traditional PINN\n",
    "        traditional_model = create_pinn_model()\n",
    "        traditional_trainer = TraditionalPINNTrainer(traditional_model, phys_params, domain_bounds)\n",
    "        \n",
    "        if train_models:\n",
    "            # Train traditional PINN\n",
    "            print(\"\\n====== Training Traditional PINN ======\")\n",
    "            traditional_loss_history = traditional_trainer.train(n_epochs=n_epochs)\n",
    "            \n",
    "            # Save traditional model\n",
    "            traditional_trainer.save_model(\"notebooks/models/traditional_pinn.h5\")\n",
    "        else:\n",
    "            # Load traditional model\n",
    "            print(\"\\n====== Loading Traditional PINN ======\")\n",
    "            traditional_trainer.load_model(\"notebooks/models/traditional_pinn.h5\") \n",
    "        \n",
    "        # Plot traditional loss history\n",
    "        traditional_trainer.plot_loss_history()\n",
    "    \n",
    "    # Create ensemble of PINNs for Entropy-Langevin\n",
    "    ensemble_size = 10\n",
    "    ensemble_models = create_model_ensemble(num_models=ensemble_size)\n",
    "    \n",
    "    # Create Entropy-Langevin trainer\n",
    "    el_trainer = EntropyLangevinPINNTrainer(ensemble_models, phys_params, domain_bounds)\n",
    "    \n",
    "    if train_models:\n",
    "        # Train Entropy-Langevin PINN\n",
    "        print(\"\\n====== Training Entropy-Langevin PINN ======\")\n",
    "        el_loss_history = el_trainer.train(n_epochs=n_epochs)\n",
    "        \n",
    "        # Save Entropy-Langevin models\n",
    "        el_trainer.save_models(\"notebooks/models/entropy_langevin_pinn.h5\")\n",
    "    else:\n",
    "        # Load Entropy-Langevin models\n",
    "        print(\"\\n====== Loading Entropy-Langevin PINN ======\")\n",
    "        el_trainer.load_models(\"notebooks/models/entropy_langevin_pinn.h5 \")\n",
    "    \n",
    "    # Plot Entropy-Langevin loss history\n",
    "    el_trainer.plot_loss_history()\n",
    "    \n",
    "    # Visualize predictions with uncertainty\n",
    "    print(\"\\n====== Visualizing Predictions ======\")\n",
    "    # Visualize SiH4 concentration\n",
    "    el_trainer.visualize_predictions(t_idx=5, output_idx=0)\n",
    "    \n",
    "    # Visualize Si concentration\n",
    "    el_trainer.visualize_predictions(t_idx=5, output_idx=1)\n",
    "    \n",
    "    # Visualize temperature\n",
    "    el_trainer.visualize_predictions(t_idx=5, output_idx=4)\n",
    "    \n",
    "    return traditional_trainer if compare_with_traditional else None, el_trainer\n",
    "\n",
    "\n",
    "demo_entropy_langevin(train_models=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7731ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c976ad6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvdenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "880857c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Documents\\UNAI_Notes\\CVD-PINN-Project\\cvdenv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Documents\\UNAI_Notes\\CVD-PINN-Project\\cvdenv\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Documents\\UNAI_Notes\\CVD-PINN-Project\\cvdenv\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n",
      "TensorFlow version: 2.19.0\n",
      "TensorFlow Probability version: 0.25.0\n",
      "No GPUs available, using CPU\n",
      "\n",
      "Physical parameters initialized:\n",
      "Diffusion coefficients: D_SiH4 = 1e-05, D_Si = 5e-06, D_H2 = 4e-05, D_SiH2 = 1.5e-05\n",
      "Thermal parameters: k = 0.1, Cp = 700.0, Ï = 1.0\n",
      "Reaction parameters: A1 = 1000000.0, E1 = 150000.0, A2 = 200000.0, E2 = 120000.0, A3 = 300000.0, E3 = 100000.0\n",
      "Gas constant: R = 8.314\n",
      "\n",
      "Domain bounds:\n",
      "x_min = 0.0\n",
      "x_max = 0.1\n",
      "y_min = 0.0\n",
      "y_max = 0.05\n",
      "t_min = 0.0\n",
      "t_max = 10.0\n",
      "\n",
      "Test input shape: (5, 3)\n",
      "Test output shape: (5, 5)\n",
      "\n",
      "Computed gradients:\n",
      "dy_dx shape: (5, 5, 3)\n",
      "y_x shape: (5, 5)\n",
      "y_y shape: (5, 5)\n",
      "y_t shape: (5, 5)\n",
      "y_xx shape: (5, 5)\n",
      "y_yy shape: (5, 5)\n",
      "\n",
      "Computed residuals:\n",
      "Residual 1 shape: (5, 1)\n",
      "Residual 2 shape: (5, 1)\n",
      "Residual 3 shape: (5, 1)\n",
      "Residual 4 shape: (5, 1)\n",
      "Residual 5 shape: (5, 1)\n",
      "\n",
      "Generated collocation points shape: (10, 3)\n",
      "Generated boundary points:\n",
      "inlet shape: (5, 3)\n",
      "substrate shape: (5, 3)\n",
      "left_wall shape: (5, 3)\n",
      "right_wall shape: (5, 3)\n",
      "Generated initial points shape: (10, 3)\n",
      "\n",
      "Entropy regularization test:\n",
      "Original loss: 1.100000023841858\n",
      "Modified loss: 1.1995666027069092\n",
      "Original gradient: 2.0\n",
      "Modified gradient: 1.040000081062317\n",
      "Updated alpha: 0.10000000149011612\n",
      "Updated beta: 10.0\n",
      "Updated alpha at halfway: 0.055000003427267075\n",
      "Updated beta at halfway: 55.0\n",
      "Updated alpha at end: 0.010000002570450306\n",
      "Updated beta at end: 100.0\n",
      "\n",
      "Notebook 1 completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "import import_ipynb\n",
    "import Mathematical_Framework_Setup as nb1\n",
    "import Entropy_Langevin_Training as nb2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "345c24d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "# Section 1: Residual-Based Adaptive Sampling\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "class ResidualBasedSampler:\n",
    "    \"\"\"\n",
    "    Class for residual-based adaptive sampling technique.\n",
    "    This focusses computational resources on regions with high PDE residual.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, pde_calculator, domain_bounds, max_points= 10000):\n",
    "        \"\"\"\n",
    "        Initialize the residual-based sampler\n",
    "\n",
    "        Args:\n",
    "            model: PINN or List of PINNs\n",
    "            The PINN model to compute residual\n",
    "            pde_calculator: CVDPDE\n",
    "            Object to compute PDEs residuals\n",
    "            domain_bounds: dict \n",
    "            Dictionary with domain bounds\n",
    "            max_points: int\n",
    "            maximum number of points to store\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.is_ensemble = isinstance(model, list)\n",
    "        self.pde_calculator = pde_calculator\n",
    "        self.domain_bounds = domain_bounds\n",
    "        self.max_points = max_points\n",
    "        \n",
    "        # Initialize points and residuals\n",
    "        self.points = None\n",
    "        self.residuals = None\n",
    "        \n",
    "    def compute_residuals(self, points):\n",
    "        \"\"\"\n",
    "        Compute PDE residuals at given points\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        points : np.ndarray or tf.Tensor\n",
    "            Points to compute residuals at\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray\n",
    "            Residual magnitudes at each point\n",
    "        \"\"\"\n",
    "        # Convert to tensor if numpy array\n",
    "        if isinstance(points, np.ndarray):\n",
    "            points = tf.convert_to_tensor(points, dtype=tf.float32)\n",
    "            \n",
    "        # Compute residuals\n",
    "        if self.is_ensemble:\n",
    "            # For ensemble, compute average residuals\n",
    "            all_residuals = []\n",
    "            \n",
    "            for model in self.model:\n",
    "                # Get model predictions\n",
    "                y_pred = model(points)\n",
    "                \n",
    "                # Get derivatives\n",
    "                derivatives = model.get_gradients(points, y_pred)\n",
    "                \n",
    "                # Compute PDE residuals\n",
    "                residuals = self.pde_calculator.compute_residuals(\n",
    "                    points, y_pred, derivatives\n",
    "                )\n",
    "                \n",
    "                # Compute total residual magnitude\n",
    "                residual_magnitude = tf.sqrt(\n",
    "                    tf.square(residuals[0]) + \n",
    "                    tf.square(residuals[1]) + \n",
    "                    tf.square(residuals[2]) + \n",
    "                    tf.square(residuals[3]) + \n",
    "                    tf.square(residuals[4])\n",
    "                )\n",
    "                \n",
    "                all_residuals.append(residual_magnitude)\n",
    "            \n",
    "            # Compute average residual\n",
    "            residual_magnitude = tf.reduce_mean(tf.stack(all_residuals, axis=0), axis=0)\n",
    "        else:\n",
    "            # For single model\n",
    "            # Get model predictions\n",
    "            y_pred = self.model(points)\n",
    "            \n",
    "            # Get derivatives\n",
    "            derivatives = self.model.get_gradients(points, y_pred)\n",
    "            \n",
    "            # Compute PDE residuals\n",
    "            residuals = self.pde_calculator.compute_residuals(\n",
    "                points, y_pred, derivatives\n",
    "            )\n",
    "            \n",
    "            # Compute total residual magnitude\n",
    "            residual_magnitude = tf.sqrt(\n",
    "                tf.square(residuals[0]) + \n",
    "                tf.square(residuals[1]) + \n",
    "                tf.square(residuals[2]) + \n",
    "                tf.square(residuals[3]) + \n",
    "                tf.square(residuals[4])\n",
    "            )\n",
    "        \n",
    "        return residual_magnitude.numpy()\n",
    "    \n",
    "    def initialize_points(self, n_points):\n",
    "        \"\"\"\n",
    "        Initialize with random points in the domain\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_points : int\n",
    "            Number of initial points\n",
    "        \"\"\"\n",
    "        # Generate random points\n",
    "        x = np.random.uniform(self.domain_bounds['x_min'], self.domain_bounds['x_max'], n_points)\n",
    "        y = np.random.uniform(self.domain_bounds['y_min'], self.domain_bounds['y_max'], n_points)\n",
    "        t = np.random.uniform(self.domain_bounds['t_min'], self.domain_bounds['t_max'], n_points)\n",
    "        \n",
    "        # Stack coordinates\n",
    "        self.points = np.stack([x, y, t], axis=1)\n",
    "        \n",
    "        # Compute residuals\n",
    "        self.residuals = self.compute_residuals(self.points)\n",
    "        \n",
    "        print(f\"Initialized {n_points} points with random sampling\")\n",
    "    \n",
    "    def get_training_points(self, n_points, method='residual_weighted'):\n",
    "        \"\"\"\n",
    "        Get training points based on residuals\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_points : int\n",
    "            Number of points to select\n",
    "        method : str\n",
    "            Sampling method: 'residual_weighted', 'top_residual', or 'mixed'\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray\n",
    "            Selected training points\n",
    "        \"\"\"\n",
    "        if self.points is None:\n",
    "            raise ValueError(\"Points not initialized. Call initialize_points() first.\")\n",
    "        \n",
    "        if method == 'residual_weighted':\n",
    "            # Normalize residuals to create a probability distribution\n",
    "            probs = self.residuals / np.sum(self.residuals)\n",
    "            \n",
    "            # Sample indices based on residual magnitudes\n",
    "            indices = np.random.choice(\n",
    "                len(self.points), \n",
    "                size=n_points, \n",
    "                replace=True,  # Allow replacement for true importance sampling\n",
    "                p=probs\n",
    "            )\n",
    "            \n",
    "            # Return selected points\n",
    "            return self.points[indices]\n",
    "        \n",
    "        elif method == 'top_residual':\n",
    "            # Select points with highest residuals\n",
    "            indices = np.argsort(self.residuals)[-n_points:]\n",
    "            \n",
    "            # Return selected points\n",
    "            return self.points[indices]\n",
    "        \n",
    "        elif method == 'mixed':\n",
    "            # Mix of random and residual-based sampling\n",
    "            n_random = n_points // 2\n",
    "            n_residual = n_points - n_random\n",
    "            \n",
    "            # Normalize residuals to create a probability distribution\n",
    "            probs = self.residuals / np.sum(self.residuals)\n",
    "            \n",
    "            # Sample indices based on residual magnitudes\n",
    "            residual_indices = np.random.choice(\n",
    "                len(self.points), \n",
    "                size=n_residual, \n",
    "                replace=True,\n",
    "                p=probs\n",
    "            )\n",
    "            \n",
    "            # Sample random indices\n",
    "            random_indices = np.random.choice(\n",
    "                len(self.points), \n",
    "                size=n_random, \n",
    "                replace=True\n",
    "            )\n",
    "            \n",
    "            # Combine indices\n",
    "            indices = np.concatenate([residual_indices, random_indices])\n",
    "            \n",
    "            # Return selected points\n",
    "            return self.points[indices]\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown sampling method: {method}\")\n",
    "    \n",
    "    def update_points(self, n_new_points, n_keep=None):\n",
    "        \"\"\"\n",
    "        Update the point database with new samples\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_new_points : int\n",
    "            Number of new points to generate\n",
    "        n_keep : int\n",
    "            Number of existing points to keep (None = keep all)\n",
    "        \"\"\"\n",
    "        # Generate new points\n",
    "        x = np.random.uniform(self.domain_bounds['x_min'], self.domain_bounds['x_max'], n_new_points)\n",
    "        y = np.random.uniform(self.domain_bounds['y_min'], self.domain_bounds['y_max'], n_new_points)\n",
    "        t = np.random.uniform(self.domain_bounds['t_min'], self.domain_bounds['t_max'], n_new_points)\n",
    "        \n",
    "        # Stack coordinates\n",
    "        new_points = np.stack([x, y, t], axis=1)\n",
    "        \n",
    "        # Compute residuals for new points\n",
    "        new_residuals = self.compute_residuals(new_points)\n",
    "        \n",
    "        # Combine with existing points\n",
    "        if self.points is not None:\n",
    "            if n_keep is not None:\n",
    "                # Keep only top n_keep points from existing database\n",
    "                indices = np.argsort(self.residuals)[-n_keep:]\n",
    "                self.points = self.points[indices]\n",
    "                self.residuals = self.residuals[indices]\n",
    "            \n",
    "            # Combine with new points\n",
    "            self.points = np.vstack([self.points, new_points])\n",
    "            self.residuals = np.concatenate([self.residuals, new_residuals])\n",
    "            \n",
    "            # Check if exceeding maximum size\n",
    "            if len(self.points) > self.max_points:\n",
    "                # Keep top max_points with highest residuals\n",
    "                indices = np.argsort(self.residuals)[-self.max_points:]\n",
    "                self.points = self.points[indices]\n",
    "                self.residuals = self.residuals[indices]\n",
    "        else:\n",
    "            # First time adding points\n",
    "            self.points = new_points\n",
    "            self.residuals = new_residuals\n",
    "        \n",
    "        print(f\"Updated point database. Now contains {len(self.points)} points.\")\n",
    "    \n",
    "    def refine_near_high_residuals(self, n_refine, n_per_point=10, refine_radius=0.01):\n",
    "        \"\"\"\n",
    "        Generate new points near locations with high residuals\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_refine : int\n",
    "            Number of high-residual points to refine around\n",
    "        n_per_point : int\n",
    "            Number of new points to generate around each high-residual point\n",
    "        refine_radius : float\n",
    "            Radius around each point for refinement\n",
    "        \"\"\"\n",
    "        if self.points is None:\n",
    "            raise ValueError(\"Points not initialized. Call initialize_points() first.\")\n",
    "        \n",
    "        # Select points with highest residuals\n",
    "        indices = np.argsort(self.residuals)[-n_refine:]\n",
    "        high_residual_points = self.points[indices]\n",
    "        \n",
    "        # Generate new points around each high-residual point\n",
    "        new_points = []\n",
    "        \n",
    "        for point in high_residual_points:\n",
    "            # Generate random perturbations\n",
    "            dx = np.random.uniform(-refine_radius, refine_radius, n_per_point)\n",
    "            dy = np.random.uniform(-refine_radius, refine_radius, n_per_point)\n",
    "            dt = np.random.uniform(-refine_radius, refine_radius, n_per_point)\n",
    "            \n",
    "            # Create new points\n",
    "            x = np.clip(point[0] + dx, self.domain_bounds['x_min'], self.domain_bounds['x_max'])\n",
    "            y = np.clip(point[1] + dy, self.domain_bounds['y_min'], self.domain_bounds['y_max'])\n",
    "            t = np.clip(point[2] + dt, self.domain_bounds['t_min'], self.domain_bounds['t_max'])\n",
    "            \n",
    "            # Stack coordinates\n",
    "            refined_points = np.stack([x, y, t], axis=1)\n",
    "            new_points.append(refined_points)\n",
    "        \n",
    "        # Combine all new points\n",
    "        new_points = np.vstack(new_points)\n",
    "        \n",
    "        # Compute residuals for new points\n",
    "        new_residuals = self.compute_residuals(new_points)\n",
    "        \n",
    "        # Combine with existing points\n",
    "        self.points = np.vstack([self.points, new_points])\n",
    "        self.residuals = np.concatenate([self.residuals, new_residuals])\n",
    "        \n",
    "        # Check if exceeding maximum size\n",
    "        if len(self.points) > self.max_points:\n",
    "            # Keep top max_points with highest residuals\n",
    "            indices = np.argsort(self.residuals)[-self.max_points:]\n",
    "            self.points = self.points[indices]\n",
    "            self.residuals = self.residuals[indices]\n",
    "        \n",
    "        print(f\"Added {len(new_points)} refined points. Now contains {len(self.points)} points.\")\n",
    "    \n",
    "    def visualize_residuals(self, t_idx=5, nx=50, ny=50, nt=10):\n",
    "        \"\"\"\n",
    "        Visualize residuals at a specific time step\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        t_idx : int\n",
    "            Time index to visualize\n",
    "        nx, ny, nt : int\n",
    "            Number of points in each dimension for visualization grid\n",
    "        \"\"\"\n",
    "        # Generate uniform grid for visualization\n",
    "        x = np.linspace(self.domain_bounds['x_min'], self.domain_bounds['x_max'], nx)\n",
    "        y = np.linspace(self.domain_bounds['y_min'], self.domain_bounds['y_max'], ny)\n",
    "        t = np.linspace(self.domain_bounds['t_min'], self.domain_bounds['t_max'], nt)\n",
    "        \n",
    "        # Create meshgrid\n",
    "        X, Y = np.meshgrid(x, y, indexing='ij')\n",
    "        \n",
    "        # Create grid points for the specific time step\n",
    "        time_val = t[t_idx]\n",
    "        grid_t = np.ones_like(X.flatten()) * time_val\n",
    "        grid_points = np.stack([X.flatten(), Y.flatten(), grid_t], axis=1)\n",
    "        \n",
    "        # Compute residuals\n",
    "        residuals = self.compute_residuals(grid_points)\n",
    "        \n",
    "        # Reshape residuals\n",
    "        residuals = residuals.reshape(X.shape)\n",
    "        \n",
    "        # Plot residuals\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # Plot residual magnitudes\n",
    "        plt.contourf(X, Y, residuals, 50, cmap='hot')\n",
    "        plt.colorbar(label='Residual Magnitude')\n",
    "        \n",
    "        # Plot high residual points (time slice)\n",
    "        time_mask = np.abs(self.points[:, 2] - time_val) < (t[1] - t[0])\n",
    "        if np.any(time_mask):\n",
    "            high_res_points = self.points[time_mask]\n",
    "            high_res_values = self.residuals[time_mask]\n",
    "            \n",
    "            # Plot top 10% residual points\n",
    "            if len(high_res_values) > 0:\n",
    "                threshold = np.percentile(high_res_values, 90)\n",
    "                mask = high_res_values > threshold\n",
    "                plt.scatter(high_res_points[mask, 0], high_res_points[mask, 1], \n",
    "                            c='k', marker='x', s=40, label='High Residual Points')\n",
    "        \n",
    "        plt.xlabel('x (m)')\n",
    "        plt.ylabel('y (m)')\n",
    "        plt.title(f'Residual Magnitude at t = {time_val:.2f}s')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'figures/residuals_t{t_idx}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "150fc3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "# Section 2: RAR-Net (Residual-Adaptive Refinement Network)\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "class RARNet:\n",
    "    \"\"\"\n",
    "    Residual-Adaptive Refinement Network (RAR-Net)\n",
    "    A PINN training strategy with adaptive point generation based on residual feedback\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, models, phys_params, domain_bounds, use_entropy_langevin=True):\n",
    "        \"\"\"\n",
    "        Initialize the RAR-Net\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        models : list or PINN\n",
    "            The PINN model(s) to train\n",
    "        phys_params : CVDPhysicalParams\n",
    "            Object containing physical parameters\n",
    "        domain_bounds : dict\n",
    "            Dictionary with domain bounds\n",
    "        use_entropy_langevin : bool\n",
    "            Whether to use Entropy-Langevin dynamics for ensemble training\n",
    "        \"\"\"\n",
    "        self.models = models\n",
    "        self.is_ensemble = isinstance(models, list)\n",
    "        self.num_models = len(models) if self.is_ensemble else 1\n",
    "        self.phys_params = phys_params\n",
    "        self.domain_bounds = domain_bounds\n",
    "        self.use_entropy_langevin = use_entropy_langevin\n",
    "        \n",
    "        # Create PDE residual calculator\n",
    "        self.pde_calculator = nb1.CVDPDE(phys_params)\n",
    "        \n",
    "        # Create data generator\n",
    "        self.data_generator = nb1.CVDDataGenerator(domain_bounds)\n",
    "        \n",
    "        # Create residual-based sampler\n",
    "        self.sampler = ResidualBasedSampler(\n",
    "            models, self.pde_calculator, domain_bounds\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer based on model type\n",
    "        if self.is_ensemble and use_entropy_langevin:\n",
    "            # Entropy-Langevin trainer for ensemble\n",
    "            self.trainer = nb2.EntropyLangevinPINNTrainer(\n",
    "                models, phys_params, domain_bounds\n",
    "            )\n",
    "        elif self.is_ensemble and not use_entropy_langevin:\n",
    "            # Create multiple traditional trainers\n",
    "            self.trainer = [\n",
    "                nb2.TraditionalPINNTrainer(model, phys_params, domain_bounds)\n",
    "                for model in models\n",
    "            ]\n",
    "        else:\n",
    "            # Traditional trainer for single model\n",
    "            self.trainer = nb2.TraditionalPINNTrainer(\n",
    "                models, phys_params, domain_bounds\n",
    "            )\n",
    "        \n",
    "        # Initialize loss history\n",
    "        self.loss_history = {\n",
    "            'total': [],\n",
    "            'pde': [],\n",
    "            'bc': [],\n",
    "            'ic': []\n",
    "        }\n",
    "        \n",
    "        # Initialize refinement history\n",
    "        self.refinement_history = {\n",
    "            'iteration': [],\n",
    "            'num_points': [],\n",
    "            'max_residual': [],\n",
    "            'avg_residual': []\n",
    "        }\n",
    "    \n",
    "    def train_with_adaptive_refinement(self, n_iterations=5, n_epochs_per_iter=1000,\n",
    "                                       initial_points=5000, refinement_points=1000,\n",
    "                                       refinement_fraction=0.2, sampling_method='mixed'):\n",
    "        \"\"\"\n",
    "        Train the PINN with adaptive refinement\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_iterations : int\n",
    "            Number of refinement iterations\n",
    "        n_epochs_per_iter : int\n",
    "            Number of training epochs per iteration\n",
    "        initial_points : int\n",
    "            Number of initial points\n",
    "        refinement_points : int\n",
    "            Number of points to add at each refinement step\n",
    "        refinement_fraction : float\n",
    "            Fraction of high-residual points to refine around\n",
    "        sampling_method : str\n",
    "            Method for sampling training points\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Training and refinement history\n",
    "        \"\"\"\n",
    "        print(\"Starting RAR-Net training with adaptive refinement...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Step 1: Initialize point database with random points\n",
    "        self.sampler.initialize_points(initial_points)\n",
    "        \n",
    "        # Generate boundary and initial condition points (these don't change)\n",
    "        boundary_points = self.data_generator.generate_boundary_points(initial_points // 10)\n",
    "        # Convert to tensors\n",
    "        boundary_points_tensor = {}\n",
    "        for key in boundary_points:\n",
    "            boundary_points_tensor[key] = tf.convert_to_tensor(boundary_points[key], dtype=tf.float32)\n",
    "        \n",
    "        initial_points_data = self.data_generator.generate_initial_points(initial_points // 10)\n",
    "        initial_points_tensor = tf.convert_to_tensor(initial_points_data, dtype=tf.float32)\n",
    "        \n",
    "        # Iterative refinement process\n",
    "        for iteration in range(n_iterations):\n",
    "            print(f\"\\n===== Refinement Iteration {iteration+1}/{n_iterations} =====\")\n",
    "            \n",
    "            # Step 2: Sample training points based on residuals\n",
    "            training_points = self.sampler.get_training_points(\n",
    "                initial_points, method=sampling_method\n",
    "            )\n",
    "            training_points_tensor = tf.convert_to_tensor(training_points, dtype=tf.float32)\n",
    "            \n",
    "            # Step 3: Train the model(s)\n",
    "            print(f\"Training with {len(training_points)} points...\")\n",
    "            \n",
    "            if self.is_ensemble and self.use_entropy_langevin:\n",
    "                # Train with Entropy-Langevin\n",
    "                loss_history = self.trainer.train(\n",
    "                    n_epochs=n_epochs_per_iter,\n",
    "                    n_collocation_points=len(training_points),\n",
    "                    print_frequency=n_epochs_per_iter // 10\n",
    "                )\n",
    "                \n",
    "                # Save loss history\n",
    "                self.loss_history['total'].extend(loss_history['total'])\n",
    "                # Handle ensemble losses differently\n",
    "                for i in range(self.num_models):\n",
    "                    if iteration == 0:\n",
    "                        self.loss_history['pde'].append(loss_history['pde'][i])\n",
    "                        self.loss_history['bc'].append(loss_history['bc'][i])\n",
    "                        self.loss_history['ic'].append(loss_history['ic'][i])\n",
    "                    else:\n",
    "                        self.loss_history['pde'][i].extend(loss_history['pde'][i])\n",
    "                        self.loss_history['bc'][i].extend(loss_history['bc'][i])\n",
    "                        self.loss_history['ic'][i].extend(loss_history['ic'][i])\n",
    "            \n",
    "            elif self.is_ensemble and not self.use_entropy_langevin:\n",
    "                # Train each model separately with traditional method\n",
    "                for i, single_trainer in enumerate(self.trainer):\n",
    "                    print(f\"Training model {i+1}/{self.num_models}...\")\n",
    "                    loss_history = single_trainer.train(\n",
    "                        n_epochs=n_epochs_per_iter,\n",
    "                        n_collocation_points=len(training_points),\n",
    "                        print_frequency=n_epochs_per_iter // 10\n",
    "                    )\n",
    "                    \n",
    "                    # Save loss history\n",
    "                    if i == 0:\n",
    "                        self.loss_history['total'].extend(loss_history['total'])\n",
    "                    \n",
    "                    if iteration == 0:\n",
    "                        self.loss_history['pde'].append(loss_history['pde'])\n",
    "                        self.loss_history['bc'].append(loss_history['bc'])\n",
    "                        self.loss_history['ic'].append(loss_history['ic'])\n",
    "                    else:\n",
    "                        self.loss_history['pde'][i].extend(loss_history['pde'])\n",
    "                        self.loss_history['bc'][i].extend(loss_history['bc'])\n",
    "                        self.loss_history['ic'][i].extend(loss_history['ic'])\n",
    "            \n",
    "            else:\n",
    "                # Train single model with traditional method\n",
    "                loss_history = self.trainer.train(\n",
    "                    n_epochs=n_epochs_per_iter,\n",
    "                    n_collocation_points=len(training_points),\n",
    "                    print_frequency=n_epochs_per_iter // 10\n",
    "                )\n",
    "                \n",
    "                # Save loss history\n",
    "                self.loss_history['total'].extend(loss_history['total'])\n",
    "                self.loss_history['pde'].extend(loss_history['pde'])\n",
    "                self.loss_history['bc'].extend(loss_history['bc'])\n",
    "                self.loss_history['ic'].extend(loss_history['ic'])\n",
    "            \n",
    "            # Step 4: Update residuals for all points\n",
    "            self.sampler.residuals = self.sampler.compute_residuals(self.sampler.points)\n",
    "            \n",
    "            # Save refinement statistics\n",
    "            self.refinement_history['iteration'].append(iteration + 1)\n",
    "            self.refinement_history['num_points'].append(len(self.sampler.points))\n",
    "            self.refinement_history['max_residual'].append(np.max(self.sampler.residuals))\n",
    "            self.refinement_history['avg_residual'].append(np.mean(self.sampler.residuals))\n",
    "            \n",
    "            # Visualize current residuals\n",
    "            self.sampler.visualize_residuals(t_idx=5)\n",
    "            \n",
    "            # Step 5: Adaptive refinement\n",
    "            # Calculate how many points to refine around\n",
    "            n_refine = int(refinement_fraction * len(self.sampler.points))\n",
    "            # Number of new points per high-residual location\n",
    "            n_per_point = int(refinement_points / n_refine) + 1\n",
    "            \n",
    "            self.sampler.refine_near_high_residuals(\n",
    "                n_refine=n_refine, \n",
    "                n_per_point=n_per_point\n",
    "            )\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"RAR-Net training completed in {total_time:.2f} seconds.\")\n",
    "        \n",
    "        # Save final model(s)\n",
    "        if self.is_ensemble and self.use_entropy_langevin:\n",
    "            self.trainer.save_models(\"models/rarnet_entropy_langevin\")\n",
    "        elif self.is_ensemble and not self.use_entropy_langevin:\n",
    "            for i, single_trainer in enumerate(self.trainer):\n",
    "                single_trainer.save_model(f\"models/rarnet_traditional_ensemble_{i}.h5\")\n",
    "        else:\n",
    "            self.trainer.save_model(\"models/rarnet_traditional.h5\")\n",
    "        \n",
    "        return {\n",
    "            'loss_history': self.loss_history,\n",
    "            'refinement_history': self.refinement_history\n",
    "        }\n",
    "    \n",
    "    def plot_refinement_history(self):\n",
    "        \"\"\"Plot the refinement history\"\"\"\n",
    "        fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        color = 'tab:blue'\n",
    "        ax1.set_xlabel('Refinement Iteration', fontsize=14)\n",
    "        ax1.set_ylabel('Number of Points', color=color, fontsize=14)\n",
    "        ax1.plot(self.refinement_history['iteration'], self.refinement_history['num_points'], \n",
    "                 'o-', color=color, linewidth=2, markersize=8)\n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "        \n",
    "        ax2 = ax1.twinx()\n",
    "        color = 'tab:red'\n",
    "        ax2.set_ylabel('Residual', color=color, fontsize=14)\n",
    "        ax2.plot(self.refinement_history['iteration'], self.refinement_history['max_residual'], \n",
    "                 's--', color='tab:red', linewidth=2, markersize=8, label='Max Residual')\n",
    "        ax2.plot(self.refinement_history['iteration'], self.refinement_history['avg_residual'], \n",
    "                 '^--', color='tab:orange', linewidth=2, markersize=8, label='Avg Residual')\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "        \n",
    "        # Add legend\n",
    "        lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax2.legend(lines1 + lines2, ['Number of Points'] + labels2, loc='upper left', fontsize=12)\n",
    "        \n",
    "        plt.title('Adaptive Refinement Progress', fontsize=16)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('figures/refinement_history.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def evaluate_on_grid(self, nx=50, ny=50, nt=10):\n",
    "        \"\"\"\n",
    "        Evaluate the model on a uniform grid and compute error statistics\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        nx, ny, nt : int\n",
    "            Number of points in each dimension\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Error statistics\n",
    "        \"\"\"\n",
    "        # Generate uniform grid\n",
    "        grid_points, grid_shape = self.data_generator.generate_uniform_grid(nx, ny, nt)\n",
    "        \n",
    "        # Compute residuals on grid\n",
    "        grid_tensor = tf.convert_to_tensor(grid_points, dtype=tf.float32)\n",
    "        grid_residuals = self.sampler.compute_residuals(grid_tensor)\n",
    "        \n",
    "        # Reshape residuals\n",
    "        grid_residuals = grid_residuals.reshape(grid_shape)\n",
    "        \n",
    "        # Compute statistics\n",
    "        max_residual = np.max(grid_residuals)\n",
    "        mean_residual = np.mean(grid_residuals)\n",
    "        median_residual = np.median(grid_residuals)\n",
    "        std_residual = np.std(grid_residuals)\n",
    "        \n",
    "        # Find location of maximum residual\n",
    "        max_idx = np.unravel_index(np.argmax(grid_residuals), grid_residuals.shape)\n",
    "        max_x = np.linspace(self.domain_bounds['x_min'], self.domain_bounds['x_max'], nx)[max_idx[0]]\n",
    "        max_y = np.linspace(self.domain_bounds['y_min'], self.domain_bounds['y_max'], ny)[max_idx[1]]\n",
    "        max_t = np.linspace(self.domain_bounds['t_min'], self.domain_bounds['t_max'], nt)[max_idx[2]]\n",
    "        \n",
    "        # Create error statistics dictionary\n",
    "        error_stats = {\n",
    "            'max_residual': max_residual,\n",
    "            'mean_residual': mean_residual,\n",
    "            'median_residual': median_residual,\n",
    "            'std_residual': std_residual,\n",
    "            'max_location': (max_x, max_y, max_t)\n",
    "        }\n",
    "        \n",
    "        print(\"\\nError Statistics on Uniform Grid:\")\n",
    "        print(f\"Max Residual: {max_residual:.6e}\")\n",
    "        print(f\"Mean Residual: {mean_residual:.6e}\")\n",
    "        print(f\"Median Residual: {median_residual:.6e}\")\n",
    "        print(f\"Std Residual: {std_residual:.6e}\")\n",
    "        print(f\"Location of Max Residual: x={max_x:.4f}, y={max_y:.4f}, t={max_t:.4f}\")\n",
    "        \n",
    "        return error_stats\n",
    "    \n",
    "    def visualize_solution(self, output_idx=0, t_idx=5, nx=50, ny=50, nt=10):\n",
    "        \"\"\"\n",
    "        Visualize the solution at a specific time step\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_idx : int\n",
    "            Output index to visualize (0: SiH4, 1: Si, 2: H2, 3: SiH2, 4: T)\n",
    "        t_idx : int\n",
    "            Time index to visualize\n",
    "        nx, ny, nt : int\n",
    "            Number of points in each dimension\n",
    "        \"\"\"\n",
    "        # Species names and titles\n",
    "        species_names = [\"SiH4\", \"Si\", \"H2\", \"SiH2\", \"Temperature\"]\n",
    "        \n",
    "        # Generate uniform grid\n",
    "        grid_points, grid_shape = self.data_generator.generate_uniform_grid(nx, ny, nt)\n",
    "        grid_tensor = tf.convert_to_tensor(grid_points, dtype=tf.float32)\n",
    "        \n",
    "        # Compute predictions\n",
    "        if self.is_ensemble:\n",
    "            # Compute ensemble predictions\n",
    "            predictions = []\n",
    "            for model in self.models:\n",
    "                pred = model(grid_tensor).numpy()\n",
    "                predictions.append(pred)\n",
    "            \n",
    "            # Stack predictions\n",
    "            predictions = np.stack(predictions, axis=0)\n",
    "            \n",
    "            # Compute mean and standard deviation\n",
    "            mean_pred = np.mean(predictions, axis=0)\n",
    "            std_pred = np.std(predictions, axis=0)\n",
    "            \n",
    "            # Reshape predictions\n",
    "            mean_pred = mean_pred.reshape(*grid_shape, 5)\n",
    "            std_pred = std_pred.reshape(*grid_shape, 5)\n",
    "            \n",
    "            # Extract specific output and time step\n",
    "            mean_slice = mean_pred[:, :, t_idx, output_idx]\n",
    "            std_slice = std_pred[:, :, t_idx, output_idx]\n",
    "            \n",
    "            # Create visualization\n",
    "            x = np.linspace(self.domain_bounds['x_min'], self.domain_bounds['x_max'], nx)\n",
    "            y = np.linspace(self.domain_bounds['y_min'], self.domain_bounds['y_max'], ny)\n",
    "            t = np.linspace(self.domain_bounds['t_min'], self.domain_bounds['t_max'], nt)\n",
    "            \n",
    "            X, Y = np.meshgrid(x, y, indexing='ij')\n",
    "            time_val = t[t_idx]\n",
    "            \n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "            \n",
    "            # Plot mean prediction\n",
    "            cf1 = ax1.contourf(X, Y, mean_slice, 50, cmap='viridis')\n",
    "            plt.colorbar(cf1, ax=ax1, label=species_names[output_idx])\n",
    "            ax1.set_xlabel('x (m)')\n",
    "            ax1.set_ylabel('y (m)')\n",
    "            ax1.set_title(f'Mean {species_names[output_idx]} at t = {time_val:.2f}s')\n",
    "            \n",
    "            # Plot standard deviation\n",
    "            cf2 = ax2.contourf(X, Y, std_slice, 50, cmap='plasma')\n",
    "            plt.colorbar(cf2, ax=ax2, label=f'Std Dev of {species_names[output_idx]}')\n",
    "            ax2.set_xlabel('x (m)')\n",
    "            ax2.set_ylabel('y (m)')\n",
    "            ax2.set_title(f'Uncertainty in {species_names[output_idx]} at t = {time_val:.2f}s')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'figures/rarnet_{species_names[output_idx]}_t{t_idx}.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "        \n",
    "        else:\n",
    "            # Single model prediction\n",
    "            pred = self.models(grid_tensor).numpy()\n",
    "            \n",
    "            # Reshape prediction\n",
    "            pred = pred.reshape(*grid_shape, 5)\n",
    "            \n",
    "            # Extract specific output and time step\n",
    "            pred_slice = pred[:, :, t_idx, output_idx]\n",
    "            \n",
    "            # Create visualization\n",
    "            x = np.linspace(self.domain_bounds['x_min'], self.domain_bounds['x_max'], nx)\n",
    "            y = np.linspace(self.domain_bounds['y_min'], self.domain_bounds['y_max'], ny)\n",
    "            t = np.linspace(self.domain_bounds['t_min'], self.domain_bounds['t_max'], nt)\n",
    "            \n",
    "            X, Y = np.meshgrid(x, y, indexing='ij')\n",
    "            time_val = t[t_idx]\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            \n",
    "            cf = plt.contourf(X, Y, pred_slice, 50, cmap='viridis')\n",
    "            plt.colorbar(cf, label=species_names[output_idx])\n",
    "            plt.xlabel('x (m)')\n",
    "            plt.ylabel('y (m)')\n",
    "            plt.title(f'{species_names[output_idx]} at t = {time_val:.2f}s')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'figures/rarnet_{species_names[output_idx]}_t{t_idx}.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f73481c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook implements advanced sampling techniques for Physics-Informed Neural Networks.\n",
      "To run the demo, execute: demo_adaptive_sampling(train_models=True, use_entropy_langevin=True)\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "# Section 3: Demo and Testing\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "def demo_adaptive_sampling(train_models=True, use_entropy_langevin=True):\n",
    "    \"\"\"\n",
    "    Demonstrate RAR-Net with adaptive sampling\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_models : bool\n",
    "        Whether to train new models or load existing ones\n",
    "    use_entropy_langevin : bool\n",
    "        Whether to use Entropy-Langevin for ensemble training\n",
    "    \"\"\"\n",
    "    # Define domain bounds\n",
    "    domain_bounds = {\n",
    "        'x_min': 0.0,\n",
    "        'x_max': 0.1,\n",
    "        'y_min': 0.0,\n",
    "        'y_max': 0.05,\n",
    "        't_min': 0.0,\n",
    "        't_max': 10.0\n",
    "    }\n",
    "    \n",
    "    # Create physical parameters\n",
    "    phys_params = nb1.CVDPhysicalParams()\n",
    "    \n",
    "    # Create models\n",
    "    if use_entropy_langevin:\n",
    "        # Create ensemble of models\n",
    "        ensemble_size = 5  # Smaller ensemble for faster demo\n",
    "        models = nb1.create_model_ensemble(num_models=ensemble_size)\n",
    "    else:\n",
    "        # Create single model\n",
    "        models = nb1.create_pinn_model()\n",
    "    \n",
    "    # Create RAR-Net\n",
    "    rarnet = RARNet(models, phys_params, domain_bounds, use_entropy_langevin)\n",
    "    \n",
    "    if train_models:\n",
    "        # Train with adaptive refinement\n",
    "        history = rarnet.train_with_adaptive_refinement(\n",
    "            n_iterations=3,  # Reduced iterations for demo\n",
    "            n_epochs_per_iter=200,  # Reduced epochs for demo\n",
    "            initial_points=1000,\n",
    "            refinement_points=500,\n",
    "            refinement_fraction=0.2,\n",
    "            sampling_method='mixed'\n",
    "        )\n",
    "        \n",
    "        # Save history\n",
    "        with open('models/rarnet_history.pkl', 'wb') as f:\n",
    "            pickle.dump(history, f)\n",
    "    else:\n",
    "        # Load models and history\n",
    "        if use_entropy_langevin:\n",
    "            rarnet.trainer.load_models(\"models/rarnet_entropy_langevin.h5\")\n",
    "        else:\n",
    "            rarnet.trainer.load_model(\"models/rarnet_traditional.h5\")\n",
    "        \n",
    "        # Load history\n",
    "        try:\n",
    "            with open('models/rarnet_history.pkl', 'rb') as f:\n",
    "                history = pickle.load(f)\n",
    "                rarnet.loss_history = history['loss_history']\n",
    "                rarnet.refinement_history = history['refinement_history']\n",
    "        except:\n",
    "            print(\"No history found. Cannot visualize results without training first.\")\n",
    "            return\n",
    "    \n",
    "    # Plot refinement history\n",
    "    rarnet.plot_refinement_history()\n",
    "    \n",
    "    # Evaluate on grid\n",
    "    error_stats = rarnet.evaluate_on_grid()\n",
    "    \n",
    "    # Visualize solution for different species\n",
    "    rarnet.visualize_solution(output_idx=0)  # SiH4\n",
    "    rarnet.visualize_solution(output_idx=1)  # Si\n",
    "    rarnet.visualize_solution(output_idx=4)  # Temperature\n",
    "\n",
    "# Only execute demo if explicitly requested (to avoid long training times)\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"This notebook implements advanced sampling techniques for Physics-Informed Neural Networks.\")\n",
    "    print(\"To run the demo, execute: demo_adaptive_sampling(train_models=True, use_entropy_langevin=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a675b6de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvdenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

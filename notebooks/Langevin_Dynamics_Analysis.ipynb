{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485d68a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "# Section 1: Parameter Space Exploration with Langevin Dynamics\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "class LangevinParameterExplorer:\n",
    "    \"\"\"\n",
    "    Class for analyzing parameter space exploration with Langevin dynamics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, models, trainer):\n",
    "        \"\"\"\n",
    "        Initialize the parameter explorer\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        models : list\n",
    "            List of PINN models\n",
    "        trainer : EntropyLangevinPINNTrainer\n",
    "            Trainer object with Entropy-Langevin algorithm\n",
    "        \"\"\"\n",
    "        self.models = models\n",
    "        self.trainer = trainer\n",
    "        self.num_models = len(models)\n",
    "        \n",
    "        self.parameter_trajectories = []\n",
    "        self.parameter_statistics = []\n",
    "        \n",
    "        self.loss_trajectories = []\n",
    "    \n",
    "    def extract_parameters(self, model_idx):\n",
    "        \"\"\"\n",
    "        Extract flattened parameters from a model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model_idx : int\n",
    "            Index of the model\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray\n",
    "            Flattened parameters\n",
    "        \"\"\"\n",
    "        model = self.models[model_idx]\n",
    "        params = []\n",
    "        \n",
    "        for layer in model.layers_list:\n",
    "            weights = layer.get_weights()\n",
    "            for w in weights:\n",
    "                params.append(w.flatten())\n",
    "        \n",
    "        return np.concatenate(params)\n",
    "    \n",
    "    def record_parameter_snapshot(self):\n",
    "        \"\"\"\n",
    "        Record parameter snapshot for all models\n",
    "        \"\"\"\n",
    "        snapshot = []\n",
    "        \n",
    "        for i in range(self.num_models):\n",
    "            params = self.extract_parameters(i)\n",
    "            snapshot.append(params)\n",
    "        \n",
    "        snapshot = np.array(snapshot)\n",
    "        mean_params = np.mean(snapshot, axis=0)\n",
    "        std_params = np.std(snapshot, axis=0)\n",
    "        \n",
    "        self.parameter_statistics.append({\n",
    "            'mean': mean_params,\n",
    "            'std': std_params,\n",
    "            'min': np.min(snapshot, axis=0),\n",
    "            'max': np.max(snapshot, axis=0),\n",
    "            'median': np.median(snapshot, axis=0)\n",
    "        })\n",
    "        \n",
    "        if snapshot.shape[1] < 10000:  \n",
    "            self.parameter_trajectories.append(snapshot)\n",
    "    \n",
    "    def record_loss_snapshot(self, losses):\n",
    "        \"\"\"\n",
    "        Record loss snapshot for all models\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        losses : dict\n",
    "            Dictionary with loss values\n",
    "        \"\"\"\n",
    "        self.loss_trajectories.append(losses)\n",
    "    \n",
    "    def visualize_parameter_diversity(self, n_components=2, method='pca'):\n",
    "        \"\"\"\n",
    "        Visualize parameter diversity using dimensionality reduction\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_components : int\n",
    "            Number of components for dimensionality reduction\n",
    "        method : str\n",
    "            Method for dimensionality reduction: 'pca' or 'tsne'\n",
    "        \"\"\"\n",
    "        if len(self.parameter_trajectories) == 0:\n",
    "            raise ValueError(\"No parameter trajectories recorded.\")\n",
    "        \n",
    "        # Select a subset of snapshots (every 10th) to avoid clutter\n",
    "        snapshot_indices = np.arange(0, len(self.parameter_trajectories), 10)\n",
    "        if snapshot_indices[-1] != len(self.parameter_trajectories) - 1:\n",
    "            snapshot_indices = np.append(snapshot_indices, len(self.parameter_trajectories) - 1)\n",
    "        \n",
    "        # Extract snapshots\n",
    "        snapshots_data = [self.parameter_trajectories[i] for i in snapshot_indices]\n",
    "        \n",
    "        # Flatten data for dimensionality reduction\n",
    "        X = np.vstack([snapshot.reshape(self.num_models, -1) for snapshot in snapshots_data])\n",
    "        \n",
    "        # Apply dimensionality reduction\n",
    "        if method == 'pca':\n",
    "            reducer = PCA(n_components=n_components)\n",
    "            X_reduced = reducer.fit_transform(X)\n",
    "            print(f\"Explained variance ratio: {reducer.explained_variance_ratio_}\")\n",
    "        elif method == 'tsne':\n",
    "            reducer = TSNE(n_components=n_components, perplexity=min(30, self.num_models*len(snapshot_indices)-1))\n",
    "            X_reduced = reducer.fit_transform(X)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "        \n",
    "        # Reshape to get trajectory for each model\n",
    "        X_reduced = X_reduced.reshape(len(snapshot_indices), self.num_models, n_components)\n",
    "        \n",
    "        # Visualization\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Plot parameter space trajectories\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, self.num_models))\n",
    "        \n",
    "        for i in range(self.num_models):\n",
    "            plt.plot(X_reduced[:, i, 0], X_reduced[:, i, 1], '-', color=colors[i], \n",
    "                     alpha=0.7, linewidth=1.5, label=f'Model {i+1}')\n",
    "            plt.plot(X_reduced[0, i, 0], X_reduced[0, i, 1], 'o', color=colors[i], markersize=8)\n",
    "            plt.plot(X_reduced[-1, i, 0], X_reduced[-1, i, 1], 's', color=colors[i], markersize=8)\n",
    "        \n",
    "        # Add start and end markers for the full ensemble\n",
    "        plt.plot([], [], 'ko', markersize=8, label='Start')\n",
    "        plt.plot([], [], 'ks', markersize=8, label='End')\n",
    "        \n",
    "        plt.title(f'Parameter Space Exploration with {method.upper()} ({n_components} components)', fontsize=16)\n",
    "        plt.xlabel(f'Component 1', fontsize=14)\n",
    "        plt.ylabel(f'Component 2', fontsize=14)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'figures/parameter_space_{method}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_parameter_statistics(self):\n",
    "        \"\"\"\n",
    "        Plot statistics of parameters over training iterations\n",
    "        \"\"\"\n",
    "        if len(self.parameter_statistics) == 0:\n",
    "            raise ValueError(\"No parameter statistics recorded.\")\n",
    "        \n",
    "        # Extract statistics\n",
    "        iterations = range(1, len(self.parameter_statistics) + 1)\n",
    "        \n",
    "        # Compute aggregate statistics over all parameters\n",
    "        mean_std = [np.mean(stats['std']) for stats in self.parameter_statistics]\n",
    "        max_std = [np.max(stats['std']) for stats in self.parameter_statistics]\n",
    "        \n",
    "        max_range = [np.mean(stats['max'] - stats['min']) for stats in self.parameter_statistics]\n",
    "        \n",
    "        # Plot statistics\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "        \n",
    "        # Plot standard deviation\n",
    "        ax1.plot(iterations, mean_std, 'b-', linewidth=2, label='Mean Std Dev')\n",
    "        ax1.plot(iterations, max_std, 'r--', linewidth=1.5, label='Max Std Dev')\n",
    "        ax1.set_ylabel('Standard Deviation', fontsize=14)\n",
    "        ax1.set_title('Parameter Diversity Over Training', fontsize=16)\n",
    "        ax1.legend(fontsize=12)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot parameter range\n",
    "        ax2.plot(iterations, max_range, 'g-', linewidth=2, label='Mean Parameter Range')\n",
    "        ax2.set_xlabel('Training Iteration', fontsize=14)\n",
    "        ax2.set_ylabel('Parameter Range', fontsize=14)\n",
    "        ax2.legend(fontsize=12)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('figures/parameter_statistics.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_loss_correlation(self):\n",
    "        \"\"\"\n",
    "        Plot correlation between parameter diversity and loss\n",
    "        \"\"\"\n",
    "        if len(self.parameter_statistics) == 0 or len(self.loss_trajectories) == 0:\n",
    "            raise ValueError(\"No parameter statistics or loss trajectories recorded.\")\n",
    "        \n",
    "        # Ensure same length\n",
    "        n = min(len(self.parameter_statistics), len(self.loss_trajectories))\n",
    "        \n",
    "        # Extract statistics\n",
    "        iterations = range(1, n + 1)\n",
    "        \n",
    "        # Parameter diversity metrics\n",
    "        mean_std = [np.mean(self.parameter_statistics[i]['std']) for i in range(n)]\n",
    "        \n",
    "        # Loss metrics (assuming total loss is available)\n",
    "        total_loss = [np.mean(self.loss_trajectories[i]['total']) for i in range(n)]\n",
    "        \n",
    "        # Create scatter plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        plt.scatter(mean_std, total_loss, c=iterations, cmap='viridis', \n",
    "                   s=50, alpha=0.8, edgecolors='k', linewidths=0.5)\n",
    "        \n",
    "        plt.colorbar(label='Training Iteration')\n",
    "        plt.xlabel('Mean Parameter Standard Deviation', fontsize=14)\n",
    "        plt.ylabel('Mean Total Loss', fontsize=14)\n",
    "        plt.title('Correlation Between Parameter Diversity and Loss', fontsize=16)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add trend line\n",
    "        z = np.polyfit(mean_std, total_loss, 1)\n",
    "        p = np.poly1d(z)\n",
    "        plt.plot(sorted(mean_std), p(sorted(mean_std)), 'r--', linewidth=2)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('figures/diversity_loss_correlation.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "# Section 2: Entropy-Langevin Hyperparameter Analysis\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "class EntropyLangevinAnalyzer:\n",
    "    \"\"\"\n",
    "    Class for analyzing the effects of Entropy-Langevin hyperparameters\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, domain_bounds, phys_params):\n",
    "        \"\"\"\n",
    "        Initialize the analyzer\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        domain_bounds : dict\n",
    "            Dictionary with domain bounds\n",
    "        phys_params : CVDPhysicalParams\n",
    "            Object containing physical parameters\n",
    "        \"\"\"\n",
    "        self.domain_bounds = domain_bounds\n",
    "        self.phys_params = phys_params\n",
    "        \n",
    "        # Results storage\n",
    "        self.results = {\n",
    "            'alpha': [],\n",
    "            'beta': [],\n",
    "            'ensemble_size': [],\n",
    "            'final_loss': [],\n",
    "            'param_diversity': [],\n",
    "            'convergence_rate': []\n",
    "        }\n",
    "    \n",
    "    def run_experiment(self, alpha_values, beta_values, ensemble_sizes, n_epochs=500, n_points=1000):\n",
    "        \"\"\"\n",
    "        Run experiments with different hyperparameter values\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        alpha_values : list\n",
    "            List of alpha values to test\n",
    "        beta_values : list\n",
    "            List of beta values to test\n",
    "        ensemble_sizes : list\n",
    "            List of ensemble sizes to test\n",
    "        n_epochs : int\n",
    "            Number of epochs for each experiment\n",
    "        n_points : int\n",
    "            Number of training points\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Experiment results\n",
    "        \"\"\"\n",
    "        print(\"Starting Entropy-Langevin hyperparameter analysis...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create data generator once\n",
    "        data_generator = CVDDataGenerator(self.domain_bounds)\n",
    "        \n",
    "        # Generate training data once\n",
    "        x_collocation = data_generator.generate_collocation_points(n_points)\n",
    "        x_collocation = tf.convert_to_tensor(x_collocation, dtype=tf.float32)\n",
    "        \n",
    "        boundary_points = data_generator.generate_boundary_points(n_points // 10)\n",
    "        # Convert to tensors\n",
    "        boundary_points_tensor = {}\n",
    "        for key in boundary_points:\n",
    "            boundary_points_tensor[key] = tf.convert_to_tensor(boundary_points[key], dtype=tf.float32)\n",
    "        \n",
    "        initial_points = data_generator.generate_initial_points(n_points // 10)\n",
    "        initial_points_tensor = tf.convert_to_tensor(initial_points, dtype=tf.float32)\n",
    "        \n",
    "        # Iterate over all combinations\n",
    "        total_experiments = len(alpha_values) * len(beta_values) * len(ensemble_sizes)\n",
    "        experiment_count = 0\n",
    "        \n",
    "        for alpha in alpha_values:\n",
    "            for beta in beta_values:\n",
    "                for ensemble_size in ensemble_sizes:\n",
    "                    experiment_count += 1\n",
    "                    print(f\"\\nExperiment {experiment_count}/{total_experiments}: \"\n",
    "                          f\"alpha={alpha}, beta={beta}, ensemble_size={ensemble_size}\")\n",
    "                    \n",
    "                    # Create models\n",
    "                    models = create_model_ensemble(num_models=ensemble_size)\n",
    "                    \n",
    "                    # Create trainer\n",
    "                    trainer = EntropyLangevinPINNTrainer(\n",
    "                        models, self.phys_params, self.domain_bounds,\n",
    "                        alpha=alpha, beta=beta, learning_rate=1e-3\n",
    "                    )\n",
    "                    \n",
    "                    # Create parameter explorer\n",
    "                    explorer = LangevinParameterExplorer(models, trainer)\n",
    "                    \n",
    "                    # Record initial parameter snapshot\n",
    "                    explorer.record_parameter_snapshot()\n",
    "                    \n",
    "                    # Train with small number of epochs to save time\n",
    "                    print(f\"Training for {n_epochs} epochs...\")\n",
    "                    \n",
    "                    # Epochs loop\n",
    "                    losses_history = []\n",
    "                    for epoch in range(n_epochs):\n",
    "                        # Update entropy-Langevin parameters\n",
    "                        trainer.entropy_reg.update_parameters(epoch, n_epochs)\n",
    "                        \n",
    "                        # Perform one training step\n",
    "                        total_losses, pde_losses, bc_losses, ic_losses = trainer.train_step(\n",
    "                            epoch, x_collocation, boundary_points_tensor, initial_points_tensor\n",
    "                        )\n",
    "                        \n",
    "                        avg_total_loss = tf.reduce_mean(total_losses).numpy()\n",
    "                        losses_history.append(avg_total_loss)\n",
    "                        \n",
    "                        if (epoch + 1) % (n_epochs // 10) == 0:\n",
    "                            print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {avg_total_loss:.6e}\")\n",
    "                            \n",
    "                            # Record parameter snapshot every 10% of epochs\n",
    "                            explorer.record_parameter_snapshot()\n",
    "                            explorer.record_loss_snapshot({\n",
    "                                'total': total_losses.numpy(),\n",
    "                                'pde': pde_losses.numpy(),\n",
    "                                'bc': bc_losses.numpy(),\n",
    "                                'ic': ic_losses.numpy()\n",
    "                            })\n",
    "                    \n",
    "                    # Measure final loss\n",
    "                    final_loss = losses_history[-1]\n",
    "                    \n",
    "                    # Measure parameter diversity\n",
    "                    param_diversity = np.mean(explorer.parameter_statistics[-1]['std'])\n",
    "                    \n",
    "                    # Measure convergence rate (simplified as loss reduction over time)\n",
    "                    initial_loss = losses_history[0]\n",
    "                    convergence_rate = (initial_loss - final_loss) / initial_loss if initial_loss > 0 else 0\n",
    "                    \n",
    "                    # Store results\n",
    "                    self.results['alpha'].append(alpha)\n",
    "                    self.results['beta'].append(beta)\n",
    "                    self.results['ensemble_size'].append(ensemble_size)\n",
    "                    self.results['final_loss'].append(final_loss)\n",
    "                    self.results['param_diversity'].append(param_diversity)\n",
    "                    self.results['convergence_rate'].append(convergence_rate)\n",
    "                    \n",
    "                    # Save parameter explorer for later analysis\n",
    "                    with open(f'models/explorer_a{alpha}_b{beta}_e{ensemble_size}.pkl', 'wb') as f:\n",
    "                        pickle.dump(explorer, f)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"Analysis completed in {total_time:.2f} seconds.\")\n",
    "        \n",
    "        # Save results\n",
    "        with open('models/hyperparameter_analysis.pkl', 'wb') as f:\n",
    "            pickle.dump(self.results, f)\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def visualize_results(self):\n",
    "        \"\"\"\n",
    "        Visualize hyperparameter analysis results\n",
    "        \"\"\"\n",
    "        if len(self.results['alpha']) == 0:\n",
    "            try:\n",
    "                with open('models/hyperparameter_analysis.pkl', 'rb') as f:\n",
    "                    self.results = pickle.load(f)\n",
    "            except:\n",
    "                raise ValueError(\"No results found. Run experiments first.\")\n",
    "        \n",
    "        # Convert to DataFrame for easier analysis\n",
    "        df = pd.DataFrame(self.results)\n",
    "        \n",
    "        # Visualize the effect of alpha and beta on final loss\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        pivot_table = df.pivot_table(\n",
    "            index='alpha', columns='beta', values='final_loss', aggfunc='mean'\n",
    "        )\n",
    "        sns.heatmap(pivot_table, annot=True, cmap='viridis_r', fmt='.2e',\n",
    "                   cbar_kws={'label': 'Final Loss'})\n",
    "        plt.title('Effect of Alpha and Beta on Final Loss', fontsize=16)\n",
    "        plt.xlabel('Beta (Inverse Temperature)', fontsize=14)\n",
    "        plt.ylabel('Alpha (Entropy Weight)', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('figures/alpha_beta_loss.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Visualize the effect of alpha and beta on parameter diversity\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        pivot_table = df.pivot_table(\n",
    "            index='alpha', columns='beta', values='param_diversity', aggfunc='mean'\n",
    "        )\n",
    "        sns.heatmap(pivot_table, annot=True, cmap='plasma', fmt='.4f',\n",
    "                   cbar_kws={'label': 'Parameter Diversity'})\n",
    "        plt.title('Effect of Alpha and Beta on Parameter Diversity', fontsize=16)\n",
    "        plt.xlabel('Beta (Inverse Temperature)', fontsize=14)\n",
    "        plt.ylabel('Alpha (Entropy Weight)', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('figures/alpha_beta_diversity.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Visualize the effect of ensemble size on convergence rate\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='ensemble_size', y='convergence_rate', data=df)\n",
    "        plt.title('Effect of Ensemble Size on Convergence Rate', fontsize=16)\n",
    "        plt.xlabel('Ensemble Size', fontsize=14)\n",
    "        plt.ylabel('Convergence Rate', fontsize=14)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('figures/ensemble_convergence.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Scatter plot of parameter diversity vs final loss\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        scatter = plt.scatter(df['param_diversity'], df['final_loss'], \n",
    "                             c=df['alpha'], s=df['ensemble_size']*10, \n",
    "                             alpha=0.7, cmap='viridis', edgecolors='k', linewidths=0.5)\n",
    "        plt.colorbar(scatter, label='Alpha Value')\n",
    "        plt.xlabel('Parameter Diversity', fontsize=14)\n",
    "        plt.ylabel('Final Loss', fontsize=14)\n",
    "        plt.title('Relationship Between Parameter Diversity and Final Loss', fontsize=16)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "        \n",
    "        # Add trend line\n",
    "        z = np.polyfit(np.log(df['param_diversity']), np.log(df['final_loss']), 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_trend = np.logspace(np.log10(df['param_diversity'].min()), \n",
    "                             np.log10(df['param_diversity'].max()), 100)\n",
    "        y_trend = np.exp(p(np.log(x_trend)))\n",
    "        plt.plot(x_trend, y_trend, 'r--', linewidth=2)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('figures/diversity_loss_relationship.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "# Section 3: Fokker-Planck Analysis of Parameter Distributions\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "class FokkerPlanckAnalyzer:\n",
    "    \"\"\"\n",
    "    Class for analyzing parameter distributions through Fokker-Planck lens\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, parameter_explorer):\n",
    "        \"\"\"\n",
    "        Initialize the Fokker-Planck analyzer\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        parameter_explorer : LangevinParameterExplorer\n",
    "            Object with parameter trajectories\n",
    "        \"\"\"\n",
    "        self.explorer = parameter_explorer\n",
    "        \n",
    "        # Check if we have trajectory data\n",
    "        if len(self.explorer.parameter_trajectories) == 0:\n",
    "            raise ValueError(\"No parameter trajectories available in the explorer.\")\n",
    "    \n",
    "    def compute_kernel_density(self, snapshot_idx, param_idx, bandwidth=0.1):\n",
    "        \"\"\"\n",
    "        Compute kernel density estimation for parameter distribution\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        snapshot_idx : int\n",
    "            Index of the parameter snapshot\n",
    "        param_idx : int\n",
    "            Index of parameter subset to analyze\n",
    "        bandwidth : float\n",
    "            Bandwidth for kernel density estimation\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            (x_grid, density_values)\n",
    "        \"\"\"\n",
    "        # Extract parameters from snapshot\n",
    "        params = self.explorer.parameter_trajectories[snapshot_idx]\n",
    "        \n",
    "        # Extract specific parameter across all models\n",
    "        param_values = params[:, param_idx]\n",
    "        \n",
    "        # Use TensorFlow Probability for KDE\n",
    "        kde = tfp.distributions.KernelDensity(\n",
    "            bandwidth=bandwidth,\n",
    "            kernel='gaussian',\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "        # Fit KDE\n",
    "        kde = kde.fit(param_values[:, np.newaxis])\n",
    "        \n",
    "        # Create grid for evaluation\n",
    "        min_val = np.min(param_values) - 2 * bandwidth\n",
    "        max_val = np.max(param_values) + 2 * bandwidth\n",
    "        x_grid = np.linspace(min_val, max_val, 1000)\n",
    "        \n",
    "        # Evaluate KDE\n",
    "        log_density = kde.log_prob(x_grid[:, np.newaxis])\n",
    "        density = tf.exp(log_density).numpy()\n",
    "        \n",
    "        return x_grid, density\n",
    "    \n",
    "    def visualize_parameter_evolution(self, param_idx=0, n_snapshots=5):\n",
    "        \"\"\"\n",
    "        Visualize evolution of parameter distribution over time\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        param_idx : int\n",
    "            Index of parameter subset to analyze\n",
    "        n_snapshots : int\n",
    "            Number of snapshots to visualize\n",
    "        \"\"\"\n",
    "        # Select snapshots (evenly spaced)\n",
    "        total_snapshots = len(self.explorer.parameter_trajectories)\n",
    "        snapshot_indices = np.linspace(0, total_snapshots-1, n_snapshots, dtype=int)\n",
    "        \n",
    "        # Set up figure\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Colors for different snapshots\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, n_snapshots))\n",
    "        \n",
    "        # Compute and plot KDEs for each snapshot\n",
    "        for i, idx in enumerate(snapshot_indices):\n",
    "            x_grid, density = self.compute_kernel_density(idx, param_idx)\n",
    "            \n",
    "            # Scale density for better visualization\n",
    "            scaled_density = density / np.max(density)\n",
    "            \n",
    "            plt.plot(x_grid, scaled_density, '-', color=colors[i], linewidth=2, \n",
    "                    label=f'Iteration {idx+1}')\n",
    "            \n",
    "            # Add scatter plot of actual parameter values\n",
    "            params = self.explorer.parameter_trajectories[idx][:, param_idx]\n",
    "            plt.scatter(params, np.zeros_like(params) + 0.05*i, color=colors[i], \n",
    "                       alpha=0.7, s=30, marker='|')\n",
    "        \n",
    "        plt.xlabel('Parameter Value', fontsize=14)\n",
    "        plt.ylabel('Normalized Density', fontsize=14)\n",
    "        plt.title(f'Evolution of Parameter Distribution Over Training', fontsize=16)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'figures/parameter_evolution_p{param_idx}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def analyze_fokker_planck_dynamics(self, param_idx=0):\n",
    "        \"\"\"\n",
    "        Analyze parameter dynamics through Fokker-Planck equation\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        param_idx : int\n",
    "            Index of parameter subset to analyze\n",
    "        \"\"\"\n",
    "        # Need at least 3 snapshots for drift and diffusion estimation\n",
    "        if len(self.explorer.parameter_trajectories) < 3:\n",
    "            raise ValueError(\"Need at least 3 snapshots for drift and diffusion estimation.\")\n",
    "        \n",
    "        # Extract parameter values for all snapshots\n",
    "        param_values = []\n",
    "        for snapshot in self.explorer.parameter_trajectories:\n",
    "            param_values.append(snapshot[:, param_idx])\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        param_values = np.array(param_values)\n",
    "        \n",
    "        # Compute drift (first moment)\n",
    "        # Drift = E[dx/dt] ≈ (x_{t+1} - x_t) / dt\n",
    "        drift = np.zeros(len(param_values) - 1)\n",
    "        for t in range(len(param_values) - 1):\n",
    "            drift[t] = np.mean(param_values[t+1] - param_values[t])\n",
    "        \n",
    "        # Compute diffusion (second moment)\n",
    "        # Diffusion = E[(dx/dt)²] / 2 ≈ (x_{t+1} - x_t)² / (2*dt)\n",
    "        diffusion = np.zeros(len(param_values) - 1)\n",
    "        for t in range(len(param_values) - 1):\n",
    "            diffusion[t] = np.mean((param_values[t+1] - param_values[t])**2) / 2\n",
    "        \n",
    "        # Use mean parameter value for each snapshot\n",
    "        mean_param_values = np.mean(param_values, axis=1)\n",
    "        \n",
    "        # Plot drift and diffusion vs parameter value\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(range(1, len(drift)+1), drift, 'b-o', linewidth=2, markersize=6)\n",
    "        plt.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "        plt.xlabel('Training Iteration', fontsize=14)\n",
    "        plt.ylabel('Drift Term', fontsize=14)\n",
    "        plt.title('Drift Term in Fokker-Planck Equation', fontsize=16)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(range(1, len(diffusion)+1), diffusion, 'r-o', linewidth=2, markersize=6)\n",
    "        plt.xlabel('Training Iteration', fontsize=14)\n",
    "        plt.ylabel('Diffusion Term', fontsize=14)\n",
    "        plt.title('Diffusion Term in Fokker-Planck Equation', fontsize=16)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'figures/fokker_planck_p{param_idx}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot drift and diffusion together to show relationship\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.scatter(drift, diffusion, c=range(len(drift)), cmap='viridis', \n",
    "                   s=80, alpha=0.8, edgecolors='k', linewidths=0.5)\n",
    "        plt.colorbar(label='Training Iteration')\n",
    "        plt.xlabel('Drift Term', fontsize=14)\n",
    "        plt.ylabel('Diffusion Term', fontsize=14)\n",
    "        plt.title('Relationship Between Drift and Diffusion in Parameter Space', fontsize=16)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add trend line\n",
    "        if len(drift) > 1:\n",
    "            z = np.polyfit(drift, diffusion, 1)\n",
    "            p = np.poly1d(z)\n",
    "            plt.plot(sorted(drift), p(sorted(drift)), 'r--', linewidth=2)\n",
    "            \n",
    "            # Add correlation coefficient\n",
    "            corr_coef = np.corrcoef(drift, diffusion)[0, 1]\n",
    "            plt.annotate(f'Correlation: {corr_coef:.4f}', \n",
    "                        xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "                        fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"k\", alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'figures/drift_diffusion_relation_p{param_idx}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "# Section 4: Demo and Testing\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "def demo_langevin_analysis(load_existing=True):\n",
    "    \"\"\"\n",
    "    Demonstrate Langevin dynamics and entropy regularization analysis\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    load_existing : bool\n",
    "        Whether to load existing data or run new experiments\n",
    "    \"\"\"\n",
    "    # Define domain bounds\n",
    "    domain_bounds = {\n",
    "        'x_min': 0.0,\n",
    "        'x_max': 0.1,\n",
    "        'y_min': 0.0,\n",
    "        'y_max': 0.05,\n",
    "        't_min': 0.0,\n",
    "        't_max': 10.0\n",
    "    }\n",
    "    \n",
    "    # Create physical parameters\n",
    "    phys_params = CVDPhysicalParams()\n",
    "    \n",
    "    if load_existing:\n",
    "        try:\n",
    "            # Load explorer from file\n",
    "            with open('models/explorer_example.pkl', 'rb') as f:\n",
    "                explorer = pickle.load(f)\n",
    "            print(\"Loaded existing parameter explorer data.\")\n",
    "        except:\n",
    "            print(\"No existing explorer data found. Running a small experiment to generate data.\")\n",
    "            load_existing = False\n",
    "    \n",
    "    if not load_existing:\n",
    "        # Create models\n",
    "        ensemble_size = 5\n",
    "        models = create_model_ensemble(num_models=ensemble_size)\n",
    "        \n",
    "        # Create trainer\n",
    "        trainer = EntropyLangevinPINNTrainer(\n",
    "            models, phys_params, domain_bounds,\n",
    "            alpha=0.2, beta=5.0, learning_rate=1e-3\n",
    "        )\n",
    "        \n",
    "        # Create parameter explorer\n",
    "        explorer = LangevinParameterExplorer(models, trainer)\n",
    "        \n",
    "        # Run a small training experiment\n",
    "        print(\"Running a small training experiment to collect parameter data...\")\n",
    "        n_epochs = 200\n",
    "        \n",
    "        # Create data generator\n",
    "        data_generator = CVDDataGenerator(domain_bounds)\n",
    "        \n",
    "        # Generate training data\n",
    "        x_collocation = data_generator.generate_collocation_points(1000)\n",
    "        x_collocation = tf.convert_to_tensor(x_collocation, dtype=tf.float32)\n",
    "        \n",
    "        boundary_points = data_generator.generate_boundary_points(100)\n",
    "        # Convert to tensors\n",
    "        boundary_points_tensor = {}\n",
    "        for key in boundary_points:\n",
    "            boundary_points_tensor[key] = tf.convert_to_tensor(boundary_points[key], dtype=tf.float32)\n",
    "        \n",
    "        initial_points = data_generator.generate_initial_points(100)\n",
    "        initial_points_tensor = tf.convert_to_tensor(initial_points, dtype=tf.float32)\n",
    "        \n",
    "        # Record initial parameter snapshot\n",
    "        explorer.record_parameter_snapshot()\n",
    "        \n",
    "        # Train and record parameter snapshots\n",
    "        for epoch in range(n_epochs):\n",
    "            # Update entropy-Langevin parameters\n",
    "            trainer.entropy_reg.update_parameters(epoch, n_epochs)\n",
    "            \n",
    "            # Perform one training step\n",
    "            total_losses, pde_losses, bc_losses, ic_losses = trainer.train_step(\n",
    "                epoch, x_collocation, boundary_points_tensor, initial_points_tensor\n",
    "            )\n",
    "            \n",
    "            avg_total_loss = tf.reduce_mean(total_losses).numpy()\n",
    "            \n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {avg_total_loss:.6e}\")\n",
    "                \n",
    "                # Record parameter snapshot\n",
    "                explorer.record_parameter_snapshot()\n",
    "                explorer.record_loss_snapshot({\n",
    "                    'total': total_losses.numpy(),\n",
    "                    'pde': pde_losses.numpy(),\n",
    "                    'bc': bc_losses.numpy(),\n",
    "                    'ic': ic_losses.numpy()\n",
    "                })\n",
    "        \n",
    "        # Save explorer for future use\n",
    "        with open('models/explorer_example.pkl', 'wb') as f:\n",
    "            pickle.dump(explorer, f)\n",
    "    \n",
    "    # Analyze parameter space exploration\n",
    "    print(\"\\nAnalyzing parameter space exploration...\")\n",
    "    explorer.visualize_parameter_diversity(method='pca')\n",
    "    explorer.visualize_parameter_diversity(method='tsne')\n",
    "    explorer.plot_parameter_statistics()\n",
    "    explorer.plot_loss_correlation()\n",
    "    \n",
    "    # Analyze parameter distributions with Fokker-Planck\n",
    "    try:\n",
    "        print(\"\\nAnalyzing parameter distributions with Fokker-Planck...\")\n",
    "        fp_analyzer = FokkerPlanckAnalyzer(explorer)\n",
    "        fp_analyzer.visualize_parameter_evolution(param_idx=0)\n",
    "        fp_analyzer.visualize_parameter_evolution(param_idx=100)\n",
    "        fp_analyzer.analyze_fokker_planck_dynamics(param_idx=0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Fokker-Planck analysis: {e}\")\n",
    "    \n",
    "    # Hyperparameter analysis\n",
    "    if not load_existing:\n",
    "        print(\"\\nRunning hyperparameter analysis (limited scope for demo)...\")\n",
    "        analyzer = EntropyLangevinAnalyzer(domain_bounds, phys_params)\n",
    "        \n",
    "        # Run very limited experiments for demo purposes\n",
    "        results = analyzer.run_experiment(\n",
    "            alpha_values=[0.1, 0.2],\n",
    "            beta_values=[5.0, 10.0],\n",
    "            ensemble_sizes=[3, 5],\n",
    "            n_epochs=100,\n",
    "            n_points=500\n",
    "        )\n",
    "        \n",
    "        analyzer.visualize_results()\n",
    "    else:\n",
    "        try:\n",
    "            # Load existing results\n",
    "            analyzer = EntropyLangevinAnalyzer(domain_bounds, phys_params)\n",
    "            analyzer.visualize_results()\n",
    "        except:\n",
    "            print(\"No existing hyperparameter analysis results found.\")\n",
    "\n",
    "# Only execute demo if explicitly requested (to avoid long computation times)\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"This notebook analyzes Langevin dynamics and entropy regularization for PINNs.\")\n",
    "    print(\"To run the demo, execute: demo_langevin_analysis(load_existing=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb1ced5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76d279a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvdenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
